{"items": [{"tags": ["python", "duplicates", "unique", "python-polars"], "owner": {"account_id": 4414299, "reputation": 3875, "user_id": 3596337, "user_type": "registered", "accept_rate": 62, "profile_image": "https://www.gravatar.com/avatar/e7077a19bac20bf5bc1ffb6e2c71039a?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "keiv.fly", "link": "https://stackoverflow.com/users/3596337/keiv-fly"}, "is_answered": true, "view_count": 26972, "accepted_answer_id": 71196662, "answer_count": 2, "score": 37, "last_activity_date": 1721745946, "creation_date": 1645376234, "last_edit_date": 1721745946, "question_id": 71196661, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71196661/what-is-the-equivalent-of-dataframe-drop-duplicates-from-pandas-in-polars", "title": "What is the equivalent of `DataFrame.drop_duplicates()` from pandas in polars?", "body": "<p>What is the equivalent of <code>drop_duplicates()</code> from pandas in polars?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n    \ndf = pl.DataFrame({&quot;a&quot;:[1,1,2], &quot;b&quot;:[2,2,3], &quot;c&quot;:[1,2,3]})\ndf\n</code></pre>\n<p>Output:</p>\n<pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2506 1   \u2502\n\u2502 1   \u2506 2   \u2506 2   \u2502\n\u2502 2   \u2506 3   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>Code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.drop_duplicates([&quot;a&quot;, &quot;b&quot;])\n</code></pre>\n<p>Delivers the following error:</p>\n<pre><code># AttributeError: 'DataFrame' object has no attribute 'drop_duplicates'\n</code></pre>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 11620322, "reputation": 1530, "user_id": 8511822, "user_type": "registered", "accept_rate": 77, "profile_image": "https://i.sstatic.net/h9Xmn.png?s=256", "display_name": "rchitect-of-info", "link": "https://stackoverflow.com/users/8511822/rchitect-of-info"}, "is_answered": true, "view_count": 17619, "accepted_answer_id": 71353314, "answer_count": 4, "score": 24, "last_activity_date": 1724396654, "creation_date": 1646405308, "last_edit_date": 1703189584, "question_id": 71353113, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71353113/polars-how-to-reorder-columns-in-a-specific-order", "title": "Polars: How to reorder columns in a specific order?", "body": "<p>I cannot find how to reorder columns in a polars dataframe in the <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.html?\" rel=\"noreferrer\">polars DataFrame docs</a>.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 21225432, "reputation": 570, "user_id": 15611217, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ba9e87922ba6892c511e1d028fa12274?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "fabioklr", "link": "https://stackoverflow.com/users/15611217/fabioklr"}, "is_answered": true, "view_count": 39994, "accepted_answer_id": 71759536, "answer_count": 1, "score": 23, "last_activity_date": 1721556631, "creation_date": 1649199618, "last_edit_date": 1721555699, "question_id": 71759316, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71759316/easily-convert-string-column-to-pl-datetime-in-polars", "title": "Easily convert string column to pl.datetime in Polars", "body": "<p>Consider a Polars data frame with a column of <code>str</code> type that indicates the date in the format <code>'27 July 2020'</code>.</p>\n<p>I would like to convert this column to the <code>polars.datetime</code> type, which is distinct from the Python standard <code>datetime</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\nfrom datetime import datetime\n\ndf = pl.DataFrame({\n    &quot;id&quot;: [1, 2], \n     &quot;event_date&quot;: [&quot;27 July 2020&quot;, &quot;31 December 2020&quot;]\n})\n\ndf = df.with_columns( \n    pl.col(&quot;event_date&quot;).map_elements(lambda x: x.replace(&quot; &quot;, &quot;-&quot;))\n                        .map_elements(lambda x: datetime.strptime(x, &quot;%d-%B-%Y&quot;))\n)\n</code></pre>\n<pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 event_date          \u2502\n\u2502 --- \u2506 ---                 \u2502\n\u2502 i64 \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2020-07-27 00:00:00 \u2502\n\u2502 2   \u2506 2020-12-31 00:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>Suppose we try to process <code>df</code> further to create a new column indicating the quarter of the year an event took place.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.with_columns(\n    pl.col(&quot;event_date&quot;).map_elements(lambda x: x.month)\n                        .map_elements(lambda x: 1 if x in range(1,4) else 2 if x in range(4,7) else 3 if x in range(7,10) else 4)\n                        .alias(&quot;quarter&quot;)\n)\n</code></pre>\n<pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 event_date          \u2506 quarter \u2502\n\u2502 --- \u2506 ---                 \u2506 ---     \u2502\n\u2502 i64 \u2506 datetime[\u03bcs]        \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2020-07-27 00:00:00 \u2506 3       \u2502\n\u2502 2   \u2506 2020-12-31 00:00:00 \u2506 4       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>How would I do this in Polars without applying custom lambdas through <code>map_elements</code>?</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 11620322, "reputation": 1530, "user_id": 8511822, "user_type": "registered", "accept_rate": 77, "profile_image": "https://i.sstatic.net/h9Xmn.png?s=256", "display_name": "rchitect-of-info", "link": "https://stackoverflow.com/users/8511822/rchitect-of-info"}, "is_answered": true, "view_count": 20127, "accepted_answer_id": 71340745, "answer_count": 2, "score": 22, "last_activity_date": 1721470530, "creation_date": 1646323842, "question_id": 71340260, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71340260/polars-create-column-with-fixed-value-from-variable", "title": "Polars: Create column with fixed value from variable", "body": "<p>I have scrubbed the <code>polars</code> docs and cannot see an example of creating a column with a fixed value from a variable.  Here is what works in <code>pandas</code>:</p>\n<pre><code>df['VERSION'] = version\n</code></pre>\n<p>Thx</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 12582771, "reputation": 575, "user_id": 9152851, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f92f9c74f60cf6bab7c94243b42b6ed1?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Felix.B", "link": "https://stackoverflow.com/users/9152851/felix-b"}, "is_answered": true, "view_count": 20274, "accepted_answer_id": 73488117, "answer_count": 1, "score": 21, "last_activity_date": 1699663384, "creation_date": 1659353637, "last_edit_date": 1661433422, "question_id": 73193006, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73193006/how-to-add-a-column-to-a-polars-dataframe-using-with-columns", "title": "How to add a column to a polars DataFrame using .with_columns()", "body": "<p>I am currently creating a new column in a polars data frame using</p>\n<pre><code>predictions = [10, 20, 30, 40, 50]\ndf['predictions'] = predictions\n</code></pre>\n<p>where <code>predictions</code> is a numpy array or list containing values I computed with another tool.</p>\n<p>However, polars throws a warning, that this option will be deprecated.\nHow can the same result be achieved using <code>.with_columns()</code>?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 20142878, "reputation": 545, "user_id": 14772152, "user_type": "registered", "profile_image": "https://i.sstatic.net/JSkp7.png?s=256", "display_name": "Daycent", "link": "https://stackoverflow.com/users/14772152/daycent"}, "is_answered": true, "view_count": 22523, "accepted_answer_id": 71850319, "answer_count": 2, "score": 19, "last_activity_date": 1721470485, "creation_date": 1649804472, "last_edit_date": 1721470449, "question_id": 71850031, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71850031/polars-how-to-filter-using-in-and-not-in-like-in-sql", "title": "Polars: How to filter using &#39;in&#39; and &#39;not in&#39; like in SQL", "body": "<p>How can I achieve the equivalents of SQL's IN and NOT IN?</p>\n<p>I have a list with the required values. Here's the scenario:</p>\n<pre><code>import pandas as pd\nimport polars as pl\nexclude_fruit = [&quot;apple&quot;, &quot;orange&quot;]\n\ndf = pl.DataFrame(\n    {\n        &quot;A&quot;: [1, 2, 3, 4, 5, 6],\n        &quot;fruits&quot;: [&quot;banana&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;],\n        &quot;B&quot;: [5, 4, 3, 2, 1, 6],\n        &quot;cars&quot;: [&quot;beetle&quot;, &quot;audi&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;frog&quot;],\n        &quot;optional&quot;: [28, 300, None, 2, -30, 949],\n    }\n)\ndf.filter(~pl.select(&quot;fruits&quot;).str.contains(exclude_fruit))\ndf.filter(~pl.select(&quot;fruits&quot;).to_pandas().isin(exclude_fruit))\ndf.filter(~pl.select(&quot;fruits&quot;).isin(exclude_fruit))\n</code></pre>\n"}, {"tags": ["python", "pandas", "group-by", "python-polars"], "owner": {"account_id": 463016, "reputation": 7091, "user_id": 865662, "user_type": "registered", "accept_rate": 52, "profile_image": "https://www.gravatar.com/avatar/e2f0a2591abd6abbe073d5f3dd639d2b?s=256&d=identicon&r=PG", "display_name": "jbssm", "link": "https://stackoverflow.com/users/865662/jbssm"}, "is_answered": true, "view_count": 18568, "accepted_answer_id": 69585269, "answer_count": 1, "score": 19, "last_activity_date": 1721181159, "creation_date": 1634235133, "last_edit_date": 1721181105, "question_id": 69575496, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69575496/how-to-use-group-by-and-apply-a-custom-function-with-polars", "title": "How to use group_by and apply a custom function with Polars?", "body": "<p>I am breaking my head trying to figure out how to use <code>group_by</code> and apply a custom function using Polars.</p>\n<p>Coming from Pandas, I was using:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom scipy.stats import spearmanr\n \ndef get_score(df):\n   return spearmanr(df[&quot;prediction&quot;], df[&quot;target&quot;]).correlation\n\ndf = pd.DataFrame({\n    &quot;era&quot;: [1, 1, 1, 2, 2, 2, 5],\n    &quot;prediction&quot;: [2, 4, 5, 190, 1, 4, 1],\n    &quot;target&quot;: [1, 3, 2, 1, 43, 3, 1]\n})\n\ncorrelations = df.groupby(&quot;era&quot;).apply(get_score)\n</code></pre>\n<p>Polars has <a href=\"https://docs.pola.rs/api/python/dev/reference/dataframe/api/polars.dataframe.group_by.GroupBy.map_groups.html#polars.dataframe.group_by.GroupBy.map_groups\" rel=\"nofollow noreferrer\"><code>map_groups()</code></a> to apply a custom function over groups, which I tried:</p>\n<pre class=\"lang-py prettyprint-override\"><code>correlations = df.group_by(&quot;era&quot;).map_groups(get_score)\n</code></pre>\n<p>But this fails with the error message:</p>\n<blockquote>\n<p>'Could not get DataFrame attribute '_df'. Make sure that you return a DataFrame object.: PyErr { type: &lt;class 'AttributeError'&gt;, value: AttributeError(&quot;'float' object has no attribute '_df'&quot;), traceback: None }</p>\n</blockquote>\n<p>Any ideas?</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 7032917, "reputation": 764, "user_id": 5387991, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/dc86e0a5dabb603310a6af7f541a8dae?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "braaannigan", "link": "https://stackoverflow.com/users/5387991/braaannigan"}, "is_answered": true, "view_count": 17765, "answer_count": 6, "score": 19, "last_activity_date": 1721470280, "creation_date": 1667469353, "last_edit_date": 1721470280, "question_id": 74301064, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74301064/in-polars-how-do-i-print-all-elements-of-a-list-column", "title": "In Polars how do I print all elements of a list column?", "body": "<p>I have a Polars <code>DataFrame</code> with a list column. I want to control how many elements of a <code>pl.List</code> column are printed.</p>\n<p>I've tried <code>pl.pl.Config.set_fmt_str_lengths()</code> but this only restricts the number of elements if set to a small value, it doesn't show more elements for a large value.</p>\n<p>I'm working in Jupyterlab but I think it's a general issue.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\nN = 5\ndf = (\n    pl.DataFrame({\n        &quot;id&quot;: range(N)\n    })\n    .with_row_index(&quot;value&quot;)\n    .rolling(\n        &quot;id&quot;, period=f&quot;{N}i&quot;\n    )\n    .agg(\n        pl.col(&quot;value&quot;)\n    )\n)\n</code></pre>\n<pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 value       \u2502\n\u2502 --- \u2506 ---         \u2502\n\u2502 i64 \u2506 list[u32]   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 [0]         \u2502\n\u2502 1   \u2506 [0, 1]      \u2502\n\u2502 2   \u2506 [0, 1, 2]   \u2502\n\u2502 3   \u2506 [0, 1, \u2026 3] \u2502\n\u2502 4   \u2506 [0, 1, \u2026 4] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n"}, {"tags": ["python", "loops", "python-polars"], "owner": {"account_id": 2142798, "reputation": 2826, "user_id": 1901071, "user_type": "registered", "accept_rate": 90, "profile_image": "https://www.gravatar.com/avatar/fc4b1bc6808a1543de0a6259a67d8f2d?s=256&d=identicon&r=PG", "display_name": "John Smith", "link": "https://stackoverflow.com/users/1901071/john-smith"}, "is_answered": true, "view_count": 19896, "accepted_answer_id": 75323864, "answer_count": 3, "score": 18, "last_activity_date": 1721469927, "creation_date": 1675343745, "last_edit_date": 1721469927, "question_id": 75323747, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75323747/polars-looping-through-the-rows-in-a-dataset", "title": "Polars looping through the rows in a dataset", "body": "<p>I am trying to loop through a Polars recordset using the following code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame({\n    &quot;start_date&quot;: [&quot;2020-01-02&quot;, &quot;2020-01-03&quot;, &quot;2020-01-04&quot;],\n    &quot;Name&quot;: [&quot;John&quot;, &quot;Joe&quot;, &quot;James&quot;]\n})\n\nfor row in df.rows():\n    print(row)\n</code></pre>\n<pre><code>('2020-01-02', 'John')\n('2020-01-03', 'Joe')\n('2020-01-04', 'James')\n</code></pre>\n<p>Is there a way to specifically reference 'Name' using the named column as opposed to the index? In Pandas this would look something like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({\n    &quot;start_date&quot;: [&quot;2020-01-02&quot;, &quot;2020-01-03&quot;, &quot;2020-01-04&quot;],\n    &quot;Name&quot;: [&quot;John&quot;, &quot;Joe&quot;, &quot;James&quot;]\n})\n\nfor index, row in df.iterrows():\n    df['Name'][index]\n</code></pre>\n<pre><code>'John'\n'Joe'\n'James'\n</code></pre>\n"}, {"tags": ["python", "pandas", "python-polars"], "owner": {"account_id": 5619481, "reputation": 9438, "user_id": 4451315, "user_type": "registered", "accept_rate": 100, "profile_image": "https://www.gravatar.com/avatar/ca1f6c6db3c9da1ebd9e3bcbfed2cc23?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "ignoring_gravity", "link": "https://stackoverflow.com/users/4451315/ignoring-gravity"}, "is_answered": true, "view_count": 1388, "accepted_answer_id": 78058453, "answer_count": 6, "score": 17, "last_activity_date": 1709104169, "creation_date": 1708881729, "question_id": 78056934, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78056934/pandas-or-polars-find-index-of-previous-element-larger-than-current-one", "title": "pandas or Polars: find index of previous element larger than current one", "body": "<p>Suppose my data looks like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>data = {\n    'value': [1,9,6,7,3, 2,4,5,1,9]\n}\n</code></pre>\n<p>For each row, I would like to find the row number of the latest previous element larger than the current one.</p>\n<p>So, my expected output is:</p>\n<pre><code>[None, 0, 1, 2, 1, 1, 3, 4, 1, 0]\n</code></pre>\n<ul>\n<li>the first element <code>1</code> has no previous element, so I want <code>None</code> in the result</li>\n<li>the next element <code>9</code> is at least as large than all its previous elements, so I want <code>0</code> in the result</li>\n<li>the next element <code>6</code>, has its previous element <code>9</code> which is larger than it. The distance between them is <code>1</code>. So, I want <code>1</code> in the result here.</li>\n</ul>\n<p>I'm aware that I can do this in a loop in Python (or in C / Rust if I write an extension).</p>\n<p>My question: is it possible to solve this <strong>using entirely dataframe operations</strong>? pandas or Polars, either is fine. But only dataframe operations.</p>\n<p>So, none of the following please:</p>\n<ul>\n<li><code>apply</code></li>\n<li><code>map_elements</code></li>\n<li><code>map_rows</code></li>\n<li><code>iter_rows</code></li>\n<li>Python for loops which loop over the rows and extract elements one-by-one from the dataframes</li>\n</ul>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 19311262, "reputation": 171, "user_id": 14116918, "user_type": "registered", "profile_image": "https://graph.facebook.com/4159910894078926/picture?type=large", "display_name": "Shikha Sheoran", "link": "https://stackoverflow.com/users/14116918/shikha-sheoran"}, "is_answered": true, "view_count": 6954, "answer_count": 5, "score": 17, "last_activity_date": 1720359816, "creation_date": 1637467805, "question_id": 70051773, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70051773/polars-is-there-a-json-normalize-like-feature-in-polars", "title": "Polars : Is there a json_normalize like feature in Polars?", "body": "<p>I went through the entire documentation of Polars but couldn't find anything which could convert nested json into dataframe.</p>\n<pre><code>test = {\n  &quot;name&quot;: &quot;Ravi&quot;,\n  &quot;Subjects&quot;: {\n    &quot;Maths&quot;: 92,\n    &quot;English&quot;: 94,\n    &quot;Hindi&quot;: 98\n  }\n}\n</code></pre>\n<p>json_normalize in pandas would convert this to a dataframe by naming the columns as name, Subjects.Maths, Subjects.English and Subjects.Hindi. So is this a possibility in Polars? I did try all the functions but it always throws an error as it doesn't undersand the nested structure.</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 1605734, "reputation": 9214, "user_id": 1485877, "user_type": "registered", "accept_rate": 63, "profile_image": "https://i.sstatic.net/tkdN8.jpg?s=256", "display_name": "drhagen", "link": "https://stackoverflow.com/users/1485877/drhagen"}, "is_answered": true, "view_count": 19330, "accepted_answer_id": 71721580, "answer_count": 1, "score": 16, "last_activity_date": 1726069998, "creation_date": 1648937010, "question_id": 71721497, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71721497/extract-value-of-polars-literal", "title": "Extract value of Polars literal", "body": "<p>If I have a Polars literal, how can I extract the value?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\nexpr = pl.lit(0.5)\n\nval = float(expr)\n# TypeError: float() argument must be a string or a real number, not 'Expr'\n</code></pre>\n"}, {"tags": ["python", "concatenation", "append", "python-polars"], "owner": {"account_id": 6996309, "reputation": 365, "user_id": 5363883, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d36cf9081ca3e21a174205f12dd34a25?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "cnpryer", "link": "https://stackoverflow.com/users/5363883/cnpryer"}, "is_answered": true, "view_count": 25055, "accepted_answer_id": 71658695, "answer_count": 2, "score": 15, "last_activity_date": 1685715708, "creation_date": 1648511843, "last_edit_date": 1685715708, "question_id": 71654966, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71654966/how-can-i-append-or-concatenate-two-dataframes-in-python-polars", "title": "How can I append or concatenate two dataframes in python polars?", "body": "<p>I see it's possible to append using the series namespace (<a href=\"https://stackoverflow.com/a/70599059/5363883\">https://stackoverflow.com/a/70599059/5363883</a>). What I'm wondering is if there is a similar method for appending or concatenating DataFrames.</p>\n<p>In <code>pandas</code> historically it could be done with <code>df1.append(df2)</code>. However that method is being deprecated (if it hasn't already been deprecated) for <code>pd.concat([df1, df2])</code>.</p>\n<p>df1</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>df2</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4</td>\n<td>5</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>res</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>4</td>\n<td>5</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>"}, {"tags": ["python", "memory", "python-polars"], "owner": {"account_id": 24865433, "reputation": 223, "user_id": 18740583, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9ce9ae2d6e7b8d0884fd3135178261f3?s=256&d=identicon&r=PG", "display_name": "fvg", "link": "https://stackoverflow.com/users/18740583/fvg"}, "is_answered": true, "view_count": 12912, "answer_count": 2, "score": 15, "last_activity_date": 1668147536, "creation_date": 1649365347, "question_id": 71788877, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71788877/polars-dataframe-memory-size-in-python", "title": "Polars DataFrame memory size in Python", "body": "<p>Was wondering about the size of particular polars DataFrames.\nI tried with:</p>\n<pre><code>from sys import getsizeof\n\ngetsizeof(df)\nOut[17]: 48\ngetsizeof(df.to_pandas())\nOut[18]: 1602923950\n</code></pre>\n<p>It appears all polars df are 48 bytes? Confused.</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 27235089, "reputation": 267, "user_id": 20762114, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/077bafbdccdf14fb5afd07a99b10e7d1?s=256&d=identicon&r=PG", "display_name": "T.H Rice", "link": "https://stackoverflow.com/users/20762114/t-h-rice"}, "is_answered": true, "view_count": 7756, "accepted_answer_id": 74782121, "answer_count": 5, "score": 15, "last_activity_date": 1720521083, "creation_date": 1670901867, "last_edit_date": 1720513147, "question_id": 74779644, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74779644/mapping-a-python-dict-to-a-polars-series", "title": "Mapping a Python dict to a Polars series", "body": "<p>In Pandas we can use the <code>map</code> function to map a dict to a series to create another series with the mapped values. More generally speaking, I believe it invokes the index operator of the argument, i.e. <code>[]</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndic = { 1: 'a', 2: 'b', 3: 'c' }\n\npd.Series([1, 2, 3, 4]).map(dic) # returns [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, NaN]\n</code></pre>\n<p>I haven't found a way to do so directly in Polars, but have found a few alternatives. Would any of these be the recommended way to do so, or is there a better way?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndic = { 1: 'a', 2: 'b', 3: 'c' }\n\n# Approach 1 - map_elements\npl.Series([1, 2, 3, 4]).map_elements(lambda v: dic.get(v, None)) # returns [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, null]\n\n# Approach 2 - left join\n(\n    pl.Series([1, 2, 3, 4])\n    .alias('key')\n    .to_frame()\n    .join(\n        pl.DataFrame({\n            'key': list(dic.keys()),\n            'value': list(dic.values()),\n        }),\n        on='key', how='left',\n    )['value']\n) # returns [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, null]\n\n# Approach 3 - to pandas and back\npl.from_pandas(pl.Series([1, 2, 3, 4]).to_pandas().map(dic)) # returns [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, null]\n</code></pre>\n<p>I saw <a href=\"https://stackoverflow.com/questions/73702931/how-to-map-a-dict-of-expressions-to-a-dataframe\">this answer on mapping a dict of expressions</a> but since its chains <code>when/then/otherwise</code> it might not work well for huge dicts.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 1628817, "reputation": 2529, "user_id": 1564947, "user_type": "registered", "accept_rate": 100, "profile_image": "https://www.gravatar.com/avatar/71ba3964ea0f6279b1f7416d0b91fb72?s=256&d=identicon&r=PG", "display_name": "daviewales", "link": "https://stackoverflow.com/users/1564947/daviewales"}, "is_answered": true, "view_count": 21585, "accepted_answer_id": 71108347, "answer_count": 2, "score": 15, "last_activity_date": 1687861329, "creation_date": 1644807331, "question_id": 71106690, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71106690/polars-specify-dtypes-for-all-columns-at-once-in-read-csv", "title": "Polars: Specify dtypes for all columns at once in read_csv", "body": "<p>In <a href=\"https://www.pola.rs/\" rel=\"noreferrer\">Polars</a>, how can one specify a single dtype for all columns in <code>read_csv</code>?</p>\n<p>According to the <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html#polars.read_csv\" rel=\"noreferrer\">docs</a>, the <code>dtypes</code> argument to <code>read_csv</code> can take either a mapping (dict) in the form of <code>{'column_name': dtype}</code>, or a list of dtypes, one for each column.\nHowever, it is not clear how to specify &quot;I want all columns to be a single dtype&quot;.</p>\n<p>If you wanted all columns to be Utf-8 for example and you knew the total number of columns, you could do:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pl.read_csv('sample.csv', dtypes=[pl.Utf8]*number_of_columns)\n</code></pre>\n<p>However, this doesn't work if you don't know the total number of columns.\nIn Pandas, you could do something like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pd.read_csv('sample.csv', dtype=str)\n</code></pre>\n<p>But this doesn't work in Polars.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 24142040, "reputation": 161, "user_id": 18108118, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9b15d89d8f9b999015fc4127da6f8355?s=256&d=identicon&r=PG", "display_name": "zenelb", "link": "https://stackoverflow.com/users/18108118/zenelb"}, "is_answered": true, "view_count": 15600, "accepted_answer_id": 70974264, "answer_count": 6, "score": 14, "last_activity_date": 1720530440, "creation_date": 1643880578, "last_edit_date": 1673428091, "question_id": 70968749, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70968749/pandas-replace-equivalent-in-python-polars", "title": "Pandas REPLACE equivalent in Python Polars", "body": "<p>Is there an elegant way how to recode values in polars dataframe.</p>\n<p>For example</p>\n<pre><code>1-&gt;0, \n2-&gt;0, \n3-&gt;1... \n</code></pre>\n<p>in Pandas it is simple like that:</p>\n<pre><code>df.replace([1,2,3,4,97,98,99],[0,0,1,1,2,2,2])\n</code></pre>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 6781089, "reputation": 143, "user_id": 5221312, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/7ce11e73f3175f4bc1cddf406eb01a91?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "eoia", "link": "https://stackoverflow.com/users/5221312/eoia"}, "is_answered": true, "view_count": 34879, "accepted_answer_id": 75720073, "answer_count": 2, "score": 14, "last_activity_date": 1715079040, "creation_date": 1678699585, "last_edit_date": 1678699680, "question_id": 75720011, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75720011/add-new-column-to-polars-dataframe", "title": "Add new column to polars DataFrame", "body": "<p>Being a new user to polars coming from pandas, I have searched polars GitHub pages, user guide, stackoverflow and discord channel on how to add a new column to a polars dataframe.\nI have only found polars examples on how to add new columns based on existing ones.</p>\n<p>How should the following pandas example be converted to polars syntax?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'existing_column': [1, 2, 3]})\n\ndf['new_column'] = &quot;some text&quot;\n</code></pre>\n<p>With expected content of final dataframe:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>existing_column</th>\n<th>new_column</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>some_text</td>\n</tr>\n<tr>\n<td>2</td>\n<td>some_text</td>\n</tr>\n<tr>\n<td>3</td>\n<td>some_text</td>\n</tr>\n</tbody>\n</table>\n</div>"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 24866390, "reputation": 131, "user_id": 18741406, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4de16e4bd00d22e2816283daffc9b983?s=256&d=identicon&r=PG", "display_name": "momentlost", "link": "https://stackoverflow.com/users/18741406/momentlost"}, "is_answered": true, "view_count": 25087, "answer_count": 3, "score": 13, "last_activity_date": 1721469329, "creation_date": 1649376144, "last_edit_date": 1721468582, "question_id": 71790235, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71790235/switching-between-dtypes-within-a-dataframe", "title": "Switching between dtypes within a DataFrame", "body": "<p>I was trying to search whether there would be a way to change the dtypes for the strings with numbers easily. For example, the problem I face is as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame({&quot;foo&quot;: \n    [&quot;100CT pen&quot;, &quot;pencils 250CT&quot;, &quot;what 125CT soever&quot;, &quot;this is a thing&quot;]}\n)\n</code></pre>\n<p>I could extract and create a new column named <code>{&quot;bar&quot;: [&quot;100&quot;, &quot;250&quot;, &quot;125&quot;, &quot;&quot;]}</code>. But then I couldn't find a handy function that converts this column to Int64 or float dtypes so that the result is <code>[100, 250, 125, null]</code>.</p>\n<p>Also, vice versa. Sometimes it would be useful to have a handy function that converts the column of <code>[100, 250, 125, 0]</code> to <code>[&quot;100&quot;, &quot;250&quot;, &quot;125&quot;, &quot;0&quot;]</code>. Is it something that already exists?</p>\n"}, {"tags": ["python-polars"], "owner": {"user_type": "does_not_exist", "display_name": "user6268172"}, "is_answered": true, "view_count": 12854, "accepted_answer_id": 72293384, "answer_count": 2, "score": 13, "last_activity_date": 1720380492, "creation_date": 1652887969, "question_id": 72292048, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72292048/idiomatic-replacement-of-empty-string-with-pl-null-null-in-polars", "title": "Idiomatic replacement of empty string &#39;&#39; with pl.Null (null) in polars", "body": "<p>I have a polars DataFrame with a number of Series that look like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pl.Series(['cow', 'cat', '', 'lobster', ''])\n</code></pre>\n<p>and I'd like them to be</p>\n<pre class=\"lang-py prettyprint-override\"><code>pl.Series(['cow', 'cat', pl.Null, 'lobster', pl.Null])\n</code></pre>\n<p>A simple string replacement won't work since <code>pl.Null</code> is not of type <code>PyString</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pl.Series(['cow', 'cat', '', 'lobster', '']).str.replace('', pl.Null)\n</code></pre>\n<p>What's the idiomatic way of doing this for a <code>Series</code>/<code>DataFrame</code> in polars?</p>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"user_type": "does_not_exist", "display_name": "user6268172"}, "is_answered": true, "view_count": 5994, "accepted_answer_id": 72636610, "answer_count": 3, "score": 13, "last_activity_date": 1712976286, "creation_date": 1655304343, "last_edit_date": 1655317056, "question_id": 72633461, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72633461/sample-from-each-group-in-polars-dataframe", "title": "Sample from each group in polars dataframe?", "body": "<p>I'm looking for a function along the lines of</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.groupby('column').agg(sample(10))\n</code></pre>\n<p>so that I can take ten or so randomly-selected elements from each group.</p>\n<p>This is specifically so I can read in a LazyFrame and work with a small sample of each group as opposed to the entire dataframe.</p>\n<h2>Update:</h2>\n<p>One approximate solution is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = lf.groupby('column').agg(\n        pl.all().sample(.001)\n    )\ndf = df.explode(df.columns[1:])\n</code></pre>\n<h2>Update 2</h2>\n<p>That approximate solution is just the same as sampling the whole dataframe and doing a groupby after. No good.</p>\n"}, {"tags": ["python", "datetime", "duration", "python-polars"], "owner": {"account_id": 3098619, "reputation": 467, "user_id": 2623317, "user_type": "registered", "profile_image": "https://i.sstatic.net/R6BkZ.jpg?s=256", "display_name": "Guz", "link": "https://stackoverflow.com/users/2623317/guz"}, "is_answered": true, "view_count": 5346, "accepted_answer_id": 75438324, "answer_count": 1, "score": 13, "last_activity_date": 1721472022, "creation_date": 1676303203, "last_edit_date": 1721472022, "question_id": 75438152, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75438152/how-to-convert-time-durations-to-numeric-in-polars", "title": "How to convert time durations to numeric in polars?", "body": "<p>Is there any built-in function in <code>polars</code> or a better way to convert time durations to numeric by defining the time resolution (e.g.: days, hours, minutes)?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame({\n    &quot;from&quot;: [&quot;2023-01-01&quot;, &quot;2023-01-02&quot;, &quot;2023-01-03&quot;],\n    &quot;to&quot;: [&quot;2023-01-04&quot;, &quot;2023-01-05&quot;, &quot;2023-01-06&quot;],\n})\n</code></pre>\n<p>My current approach:</p>\n<pre><code># Convert to date and calculate the time difference\ndf = (\n    df.with_columns(\n        pl.col(&quot;to&quot;, &quot;from&quot;).str.to_date().name.suffix(&quot;_date&quot;)\n    )\n    .with_columns((pl.col(&quot;to_date&quot;) - pl.col(&quot;from_date&quot;)).alias(&quot;time_diff&quot;))\n)\n\n# Convert the time difference to int (in days)\ndf = df.with_columns(\n    ((pl.col(&quot;time_diff&quot;) / (24 * 60 * 60 * 1000)).cast(pl.Int8)).alias(&quot;time_diff_int&quot;)\n)\n</code></pre>\n<p>Output:</p>\n<pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 from       \u2506 to         \u2506 to_date    \u2506 from_date  \u2506 time_diff    \u2506 time_diff_int \u2502\n\u2502 ---        \u2506 ---        \u2506 ---        \u2506 ---        \u2506 ---          \u2506 ---           \u2502\n\u2502 str        \u2506 str        \u2506 date       \u2506 date       \u2506 duration[ms] \u2506 i8            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 \u2506 2023-01-04 \u2506 2023-01-04 \u2506 2023-01-01 \u2506 3d           \u2506 3             \u2502\n\u2502 2023-01-02 \u2506 2023-01-05 \u2506 2023-01-05 \u2506 2023-01-02 \u2506 3d           \u2506 3             \u2502\n\u2502 2023-01-03 \u2506 2023-01-06 \u2506 2023-01-06 \u2506 2023-01-03 \u2506 3d           \u2506 3             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 6645835, "reputation": 561, "user_id": 5129982, "user_type": "registered", "profile_image": "https://i.sstatic.net/xs1AZ.png?s=256", "display_name": "Alex", "link": "https://stackoverflow.com/users/5129982/alex"}, "is_answered": true, "view_count": 8024, "accepted_answer_id": 73212748, "answer_count": 2, "score": 12, "last_activity_date": 1721575224, "creation_date": 1659467562, "last_edit_date": 1721472160, "question_id": 73212628, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73212628/retrieve-date-from-datetime-column-in-polars", "title": "Retrieve date from datetime column in polars", "body": "<p>Currently when I try to retrieve date from a polars datetime column, I have to write something similar to:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\nimport datetime as dt\n\ndf = pl.DataFrame({\n    'time': [dt.datetime.now()]\n})\n\ndf = df.with_columns(\n    pl.col(&quot;time&quot;).map_elements(lambda x: x.date()).alias(&quot;date&quot;)\n)\n</code></pre>\n<pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                       \u2506 date       \u2502\n\u2502 ---                        \u2506 ---        \u2502\n\u2502 datetime[\u03bcs]               \u2506 date       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2024-07-20 11:41:04.265539 \u2506 2024-07-20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>Is there a different way, something closer to:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pl.col(&quot;time&quot;).dt.date().alias(&quot;date&quot;)\n</code></pre>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 16178990, "reputation": 143, "user_id": 11680995, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-lJ798r0Yuuo/AAAAAAAAAAI/AAAAAAAAAu8/SOq79z32qbg/photo.jpg?sz=256", "display_name": "roei shlezinger", "link": "https://stackoverflow.com/users/11680995/roei-shlezinger"}, "is_answered": true, "view_count": 21091, "accepted_answer_id": 75523731, "answer_count": 1, "score": 12, "last_activity_date": 1717036228, "creation_date": 1676998091, "last_edit_date": 1717008870, "question_id": 75523498, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75523498/python-polars-how-to-get-the-row-count-of-a-lazyframe", "title": "Python Polars: How to get the row count of a LazyFrame?", "body": "<p>The CSV file I have is 70 Gb in size. I want to load the DF and count the number of rows, in lazy mode. What's the best way to do so?</p>\n<p>As far as I can tell, there is no function like shape in lazy mode according to the documentation.\nI found this <a href=\"https://stackoverflow.com/questions/41553467/how-can-i-get-the-row-count-of-a-csv-file\">answer</a> which provide a solution not based on Polars, but I wonder if it is possible to do this in Polars as well.</p>\n"}, {"tags": ["python", "plotly-python", "python-polars"], "owner": {"account_id": 21225432, "reputation": 570, "user_id": 15611217, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ba9e87922ba6892c511e1d028fa12274?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "fabioklr", "link": "https://stackoverflow.com/users/15611217/fabioklr"}, "is_answered": true, "view_count": 9954, "accepted_answer_id": 71742617, "answer_count": 2, "score": 12, "last_activity_date": 1687624786, "creation_date": 1649077777, "last_edit_date": 1649199832, "question_id": 71737836, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71737836/how-to-use-polars-with-plotly-without-converting-to-pandas", "title": "How to use Polars with Plotly without converting to Pandas?", "body": "<p>I would like to replace Pandas with Polars but I was not able to find out how to use Polars with Plotly without converting to Pandas. I wonder if there is a way to completely cut Pandas out of the process.</p>\n<p>Consider the following test data:</p>\n<pre><code>import polars as pl\nimport numpy as np\nimport plotly.express as px\n\ndf = pl.DataFrame(\n    {\n        &quot;nrs&quot;: [1, 2, 3, None, 5],\n        &quot;names&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;, None],\n        &quot;random&quot;: np.random.rand(5),\n        &quot;groups&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],\n    }\n)\n\nfig = px.bar(df, x='names', y='random')\nfig.show()\n</code></pre>\n<p>I would like this code to show the bar chart in a Jupyter notebook but instead it returns an error:</p>\n<pre><code>/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/polars/internals/frame.py:1483: UserWarning: accessing series as Attribute of a DataFrame is deprecated\n  warnings.warn(&quot;accessing series as Attribute of a DataFrame is deprecated&quot;)\n</code></pre>\n<p>It is possible to transform the Polars data frame to a Pandas data frame with <code>df = df.to_pandas()</code>. Then, it works. However, is there another, simpler and more elegant solution?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 1605734, "reputation": 9214, "user_id": 1485877, "user_type": "registered", "accept_rate": 63, "profile_image": "https://i.sstatic.net/tkdN8.jpg?s=256", "display_name": "drhagen", "link": "https://stackoverflow.com/users/1485877/drhagen"}, "is_answered": true, "view_count": 10771, "accepted_answer_id": 71011285, "answer_count": 3, "score": 11, "last_activity_date": 1717102919, "creation_date": 1644177688, "last_edit_date": 1707822766, "question_id": 71011161, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71011161/compare-two-polars-dataframes-for-equality", "title": "Compare two polars DataFrames for equality", "body": "<p>How do I compare two polars <code>DataFrames</code> for value equality? It appears that <code>==</code> is only true if the two tables are the same object:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\npl.DataFrame({&quot;x&quot;: [1,2,3]}) == pl.DataFrame({&quot;x&quot;: [1,2,3]})  # False\n</code></pre>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 902960, "reputation": 2581, "user_id": 1064197, "user_type": "registered", "accept_rate": 89, "profile_image": "https://www.gravatar.com/avatar/65e01d911ab3d7c83bba77a4614eb060?s=256&d=identicon&r=PG", "display_name": "Michael WS", "link": "https://stackoverflow.com/users/1064197/michael-ws"}, "is_answered": true, "view_count": 5483, "accepted_answer_id": 73101672, "answer_count": 1, "score": 11, "last_activity_date": 1720379633, "creation_date": 1658691975, "question_id": 73101521, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73101521/polars-equivalent-to-pandas-groupby-shift", "title": "polars equivalent to pandas groupby shift()", "body": "<p>Is there an equivalent way to to df.groupby().shift in polars? <a href=\"https://stackoverflow.com/questions/53335567/use-pandas-shift-within-a-group\">Use pandas.shift() within a group</a></p>\n"}, {"tags": ["python", "dataframe", "data-science", "python-polars"], "owner": {"account_id": 6389621, "reputation": 3214, "user_id": 5257450, "user_type": "registered", "accept_rate": 52, "profile_image": "https://www.gravatar.com/avatar/2e0fe1ad92fe3293edf9acb85108cf25?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "xxx222", "link": "https://stackoverflow.com/users/5257450/xxx222"}, "is_answered": true, "view_count": 11763, "accepted_answer_id": 72474838, "answer_count": 2, "score": 11, "last_activity_date": 1725833309, "creation_date": 1654165576, "last_edit_date": 1720377889, "question_id": 72474673, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72474673/what-is-the-recommended-way-for-retrieving-row-numbers-index-for-polars", "title": "What is the recommended way for retrieving row numbers (index) for polars?", "body": "<p>I know polars does not support index by design, so <code>df.filter(expr).index</code> isn't an option, another way I can think of is by adding a new column before applying any filters, not sure if this is an optimal way for doing so in polars</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.with_columns(pl.Series('index', range(len(df))).filter(expr).index\n</code></pre>\n"}, {"tags": ["python", "machine-learning", "scikit-learn", "python-polars"], "owner": {"account_id": 26897236, "reputation": 389, "user_id": 20474952, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b348bc3e89590d8c1b0a41445d536458?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Regular Tech Guy", "link": "https://stackoverflow.com/users/20474952/regular-tech-guy"}, "is_answered": true, "view_count": 9493, "accepted_answer_id": 74402124, "answer_count": 4, "score": 11, "last_activity_date": 1716452642, "creation_date": 1668146395, "last_edit_date": 1716452585, "question_id": 74398563, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn", "title": "How to use polars dataframes with scikit-learn?", "body": "<p>I'm unable to use polars dataframes with scikit-learn for ML training.</p>\n<p>Currently, I'm preprocessing all dataframes in polars and convert them to pandas for model training in order for it to work.</p>\n<p>Is there any method to directly use polars dataframes with the scikit-learn API (without converting to pandas first)?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 6637266, "reputation": 669, "user_id": 5124071, "user_type": "registered", "accept_rate": 50, "profile_image": "https://i.sstatic.net/KuAcL.jpg?s=256", "display_name": "lemmingxuan", "link": "https://stackoverflow.com/users/5124071/lemmingxuan"}, "is_answered": true, "view_count": 9992, "accepted_answer_id": 72245435, "answer_count": 1, "score": 11, "last_activity_date": 1721472963, "creation_date": 1652583229, "last_edit_date": 1721472963, "question_id": 72245243, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72245243/polars-how-to-add-a-column-with-numerical", "title": "Polars: how to add a column with numerical?", "body": "<p>In pandas, we can just assign directly:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({&quot;a&quot;: [1, 2]})\n\n# add a single value\ndf[&quot;b&quot;] = 3\n\n# add an existing Series\ndf[&quot;c&quot;] = pd.Series([4, 5])\n</code></pre>\n<pre><code>   a  b  c\n0  1  3  4\n1  2  3  5\n</code></pre>\n<p>Notice that the new numerical Series is not in the original <code>df</code>, it is a result of some computation.</p>\n<p>How do we do the same thing in polars?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame({&quot;a&quot;: [1, 2]})\n\ndf = df.with_columns(...) # ????\n</code></pre>\n"}, {"tags": ["python", "apply", "python-polars"], "owner": {"account_id": 24711174, "reputation": 111, "user_id": 18601363, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/81e2409cf65f85e4f8b1fc41763167ff?s=256&d=identicon&r=PG", "display_name": "Pradeepgb", "link": "https://stackoverflow.com/users/18601363/pradeepgb"}, "is_answered": true, "view_count": 14303, "answer_count": 1, "score": 11, "last_activity_date": 1721175284, "creation_date": 1648543335, "last_edit_date": 1721175052, "question_id": 71658991, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71658991/how-to-apply-a-custom-function-in-polars-that-does-the-processing-row-by-row", "title": "How to apply a custom function in Polars that does the processing row by row?", "body": "<p>I want to pass each row of a Polars DataFrame into a custom function.</p>\n<pre class=\"lang-py prettyprint-override\"><code>def my_complicated_function(row):\n    \n    # ...\n    \n    return result\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame({\n    &quot;foo&quot;: [1, 2, 3], \n    &quot;bar&quot;: [4, 5, 6], \n    &quot;baz&quot;: [7, 8, 9]\n})\n</code></pre>\n<p>I need to process the values with some custom Python logic and want to store the result in a new column.</p>\n<pre class=\"lang-py prettyprint-override\"><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2506 baz \u2506 result   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 ---      \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4   \u2506 7   \u2506 result 1 \u2502\n\u2502 2   \u2506 5   \u2506 8   \u2506 result 2 \u2502\n\u2502 3   \u2506 6   \u2506 9   \u2506 result 3 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>In Pandas, I would use <code>df.apply(..., axis=1)</code> for this.</p>\n"}, {"tags": ["python", "dataframe", "apply", "python-polars"], "owner": {"account_id": 14521491, "reputation": 454, "user_id": 10489040, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-KTc6MZg9ODE/AAAAAAAAAAI/AAAAAAAAAag/pva1DQ9S-t8/photo.jpg?sz=256", "display_name": "Gian Arauz", "link": "https://stackoverflow.com/users/10489040/gian-arauz"}, "is_answered": true, "view_count": 12089, "accepted_answer_id": 67934527, "answer_count": 1, "score": 11, "last_activity_date": 1721565427, "creation_date": 1622799200, "last_edit_date": 1721562339, "question_id": 67834912, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67834912/apply-function-to-all-columns-of-a-polars-dataframe", "title": "Apply function to all columns of a Polars-DataFrame", "body": "<p>I know how to apply a function to all columns present in a <strong>Pandas-DataFrame</strong>. However, I have not figured out yet how to achieve this when using a <strong>Polars-DataFrame</strong>.</p>\n<p>I checked the <a href=\"https://docs.pola.rs/user-guide/expressions/user-defined-functions/\" rel=\"nofollow noreferrer\">section</a> from the <a href=\"https://docs.pola.rs/user-guide/getting-started/\" rel=\"nofollow noreferrer\">Polars User Guide</a> devoted to this topic, but I have not find the answer. Here I attach a code snippet with my unsuccessful attempts.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport polars as pl\nimport seaborn as sns\n\n# Loading toy dataset as Pandas DataFrame using Seaborn\ndf_pd = sns.load_dataset('iris')\n\n# Converting Pandas DataFrame to Polars DataFrame\ndf_pl = pl.DataFrame(df_pd)\n\n# Dropping the non-numeric column...\ndf_pd = df_pd.drop(columns='species')                     # ... using Pandas\ndf_pl = df_pl.drop('species')                             # ... using Polars\n\n# Applying function to the whole DataFrame...\ndf_pd_new = df_pd.apply(np.log2)                          # ... using Pandas\n# df_pl_new = df_pl.apply(np.log2)                        # ... using Polars?\n\n# Applying lambda function to the whole DataFrame...\ndf_pd_new = df_pd.apply(lambda c: np.log2(c))             # ... using Pandas\n# df_pl_new = df_pl.apply(lambda c: np.log2(c))           # ... using Polars?\n</code></pre>\n<p>Thanks in advance for your help and your time.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 24567951, "reputation": 295, "user_id": 18474843, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJztJdkjbqzeP3MNnSgkDYZQV6biki1aRZUw56if=k-s256", "display_name": "Hrushi", "link": "https://stackoverflow.com/users/18474843/hrushi"}, "is_answered": true, "view_count": 13451, "accepted_answer_id": 71495211, "answer_count": 2, "score": 11, "last_activity_date": 1721473972, "creation_date": 1647363042, "last_edit_date": 1655208579, "question_id": 71486019, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71486019/how-to-drop-row-in-polars-python", "title": "How to drop row in polars-python", "body": "<p>How to add new feature like length of data frame &amp; Drop rows value using indexing.\nI want to a add a new column where I can count the no-of rows available in a data frame,\n&amp; using indexing drop rows value.</p>\n<pre><code>for i in range(len(df)):\n    if (df['col1'][i] == df['col2'][i]) and (df['col4'][i] == df['col3'][i]):\n        pass\n    elif (df['col1'][i] == df['col3'][i]) and (df['col4'][i] == df['col2'][i]): \n        df['col1'][i] = df['col2'][i]\n        df['col4'][i] = df['col3'][i]\n    else:\n       df = df.drop(i)\n</code></pre>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 17758487, "reputation": 152, "user_id": 13016256, "user_type": "registered", "profile_image": "https://i.sstatic.net/GUI1E.jpg?s=256", "display_name": "Joris-Karl Huysmans", "link": "https://stackoverflow.com/users/13016256/joris-karl-huysmans"}, "is_answered": true, "view_count": 1630, "accepted_answer_id": 75691351, "answer_count": 3, "score": 11, "last_activity_date": 1721365492, "creation_date": 1678402377, "last_edit_date": 1678417447, "question_id": 75690784, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75690784/polars-for-python-how-to-get-rid-of-ensure-you-pass-a-path-to-the-file-instead", "title": "Polars for Python: How to get rid of &quot;Ensure you pass a path to the file instead of a python file object&quot; warning when reading to a dataframe?", "body": "<h3>The statement</h3>\n<ul>\n<li>I'm reading data sets using <code>Polars.read_csv()</code> method via a Python file handler:</li>\n</ul>\n<pre><code> with gzip.open(os.path.join(getParameters()['rdir'], dataset)) as compressed_file:\n    df = pl.read_csv(compressed_file, sep = '\\t', ignore_errors=True)\n</code></pre>\n<ul>\n<li>A performance warning keeps popping up:</li>\n</ul>\n<pre><code>Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance.\n</code></pre>\n<h3>Possible solutions</h3>\n<ul>\n<li>I already tried Python warning suppression, but it seems Polars literally just prints out this statement without any default warning associated.</li>\n<li>Another possibility would be to read using non-handler methods?</li>\n</ul>\n<p>Any ideas on how to get rid of this annoying message will be highly appreciated.</p>\n"}, {"tags": ["python", "pyspark", "python-polars"], "owner": {"account_id": 25954727, "reputation": 103, "user_id": 19672225, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6614578c9467be0cb1f60fd112059923?s=256&d=identicon&r=PG", "display_name": "s1nbad", "link": "https://stackoverflow.com/users/19672225/s1nbad"}, "is_answered": true, "view_count": 12020, "accepted_answer_id": 73205690, "answer_count": 4, "score": 10, "last_activity_date": 1663926583, "creation_date": 1659424112, "question_id": 73203318, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73203318/how-to-transform-spark-dataframe-to-polars-dataframe", "title": "How to transform Spark dataframe to Polars dataframe?", "body": "<p>I wonder how i can transform Spark dataframe to Polars dataframe.</p>\n<p>Let's say i have this code on PySpark:</p>\n<pre><code>df = spark.sql('''select * from tmp''')\n</code></pre>\n<p>I can easily transform it to pandas dataframe using <code>.toPandas</code>.\nIs there something similar in polars, as I need to get a polars dataframe for further processing?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 24179081, "reputation": 317, "user_id": 18140022, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyEeXTyJjuxdwf98ow9oDYn6JZIqVXkpgVjh5zk=k-s256", "display_name": "user18140022", "link": "https://stackoverflow.com/users/18140022/user18140022"}, "is_answered": true, "view_count": 11669, "accepted_answer_id": 75985318, "answer_count": 3, "score": 10, "last_activity_date": 1702988134, "creation_date": 1681209797, "question_id": 75984983, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75984983/polars-change-a-value-in-a-dataframe-if-a-condition-is-met-in-another-column", "title": "Polars: change a value in a dataframe if a condition is met in another column", "body": "<p>I have this dataframe</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>one</th>\n<th>two</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>hola</td>\n</tr>\n<tr>\n<td>b</td>\n<td>world</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>And I want to change <em>hola</em> for <em>hello</em>:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>one</th>\n<th>two</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td><strong>hello</strong></td>\n</tr>\n<tr>\n<td>b</td>\n<td>world</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>How can I change the values of a row based on a condition in another column?</p>\n<p>For instance, with PostgreSQL I could do this:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>UPDATE my_table SET two = 'hello' WHERE one = 'a';\n</code></pre>\n<p>Or in Spark</p>\n<pre class=\"lang-py prettyprint-override\"><code>my_table.withColumn(&quot;two&quot;, when(col(&quot;one&quot;) == &quot;a&quot;, &quot;hello&quot;))\n</code></pre>\n<p>I've tried using <code>with_columns(pl.when(pl.col(&quot;one&quot;) == &quot;a&quot;).then(&quot;hello&quot;))</code> but that changes the column &quot;one&quot;.</p>\n<p>EDIT: I could create a SQL instance and plot my way through via SQL but there must be way to achieve this via the Python API.</p>\n"}, {"tags": ["concatenation", "append", "python-polars"], "owner": {"account_id": 22121493, "reputation": 337, "user_id": 16374636, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0e4752e2d21c5459d396cc0fd9f57369?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "zacko", "link": "https://stackoverflow.com/users/16374636/zacko"}, "is_answered": true, "view_count": 8535, "accepted_answer_id": 72643075, "answer_count": 1, "score": 10, "last_activity_date": 1685715810, "creation_date": 1655367347, "last_edit_date": 1685715683, "question_id": 72642575, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72642575/how-can-i-concat-polars-dataframes-that-have-different-columns", "title": "How can I concat polars dataframes that have different columns", "body": "<p>In pandas it happens automatically, just by calling <code>pd.concat([df1, df2, df3])</code> and the frame that didn't have the column previously just gets a column filled with <code>NaN</code>s.</p>\n<p>In polars I get a <code>'shape error'</code> with the message that the columns differ (11 cols in <code>df1</code> vs 12 cols in <code>df2</code>).</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 8061961, "reputation": 1003, "user_id": 6077239, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d64f3a25c9fd26ad2f8984239cfa1b97?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "lebesgue", "link": "https://stackoverflow.com/users/6077239/lebesgue"}, "is_answered": true, "view_count": 3706, "accepted_answer_id": 74906705, "answer_count": 3, "score": 10, "last_activity_date": 1712360156, "creation_date": 1671763173, "last_edit_date": 1671814628, "question_id": 74895640, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74895640/how-to-do-regression-simple-linear-for-example-in-polars-select-or-groupby-con", "title": "How to do regression (simple linear for example) in polars select or groupby context?", "body": "<p>I am using polars in place of pandas. I am quite amazed by the speed and lazy computation/evaluation. Right now, there are a lot of methods on lazy dataframe, but they can only drive me so far.</p>\n<p>So, I am wondering what is the best way to use polars in combination with other tools to achieve more complicated operations, such as regression/model fitting.</p>\n<p>To be more specific, I will give an example involving linear regression.</p>\n<p>Assume I have a polars dataframe with columns day, y, x1 and x2, and I want to generate a series, which is the residual of regressing y on x1 and x2 group by day. I have included the code example as follows and how it can be solved using pandas and statsmodels. How can I get the same result with the most efficiency using idiomatic polars?</p>\n<pre><code>import pandas as pd\nimport statsmodels.api as sm\n\ndef regress_resid(df, yvar, xvars):\n    result = sm.OLS(df[yvar], sm.add_constant(df[xvars])).fit()\n    return result.resid\n\ndf = pd.DataFrame(\n    {\n        &quot;day&quot;: [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n        &quot;y&quot;: [1, 6, 3, 2, 8, 4, 5, 2, 7, 3],\n        &quot;x1&quot;: [1, 8, 2, 3, 5, 2, 1, 2, 7, 3],\n        &quot;x2&quot;: [8, 5, 3, 6, 3, 7, 3, 2, 9, 1],\n    }\n)\n\ndf.groupby(&quot;day&quot;).apply(regress_resid, &quot;y&quot;, [&quot;x1, &quot;x2])\n# day\n# 1    0    0.772431\n#      1   -0.689233\n#      2   -1.167210\n#      3   -0.827896\n#      4    1.911909\n# 2    5   -0.851691\n#      6    1.719451\n#      7   -1.167727\n#      8    0.354871\n#      9   -0.054905\n</code></pre>\n<p>Thanks for your help.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 27874805, "reputation": 113, "user_id": 21284510, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/35d0676fce514b56388b4e18f52550af?s=256&d=identicon&r=PG", "display_name": "V0N_fs", "link": "https://stackoverflow.com/users/21284510/v0n-fs"}, "is_answered": true, "view_count": 8858, "answer_count": 1, "score": 10, "last_activity_date": 1708805753, "creation_date": 1692323203, "last_edit_date": 1695927225, "question_id": 76925989, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76925989/print-all-columns-in-polars", "title": "Print all Columns in polars", "body": "<p>I need to print all the columns in my file, but the result I get is this....</p>\n<p><a href=\"https://i.sstatic.net/RbPBm.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/RbPBm.png\" alt=\"enter image description here\" /></a></p>\n<p>Do you know how I can show all the columns of my data frame?</p>\n<p>the code is this:</p>\n<pre><code>\nfile = pl.read_excel('1.xlsx')\n\nfile = file.drop_nulls()\n\nprint(file)\n</code></pre>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 65271, "reputation": 5855, "user_id": 192221, "user_type": "registered", "accept_rate": 59, "profile_image": "https://www.gravatar.com/avatar/38504ff52422b72b239a7b05ef0ccaae?s=256&d=identicon&r=PG", "display_name": "kristianp", "link": "https://stackoverflow.com/users/192221/kristianp"}, "is_answered": true, "view_count": 1899, "accepted_answer_id": 75217166, "answer_count": 1, "score": 10, "last_activity_date": 1674537748, "creation_date": 1674529729, "last_edit_date": 1674537748, "question_id": 75217063, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75217063/polars-table-convert-a-list-column-to-separate-rows-i-e-unnest-a-list-column-to", "title": "Polars table convert a list column to separate rows i.e. unnest a list column to multiple rows", "body": "<p>I have a Polars dataframe in the form:</p>\n<pre><code>df = pl.DataFrame({'a':[1,2,3], 'b':[['a','b'],['a'],['c','d']]}) \n</code></pre>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b          \u2502\n\u2502 --- \u2506 ---        \u2502\n\u2502 i64 \u2506 list[str]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 [&quot;a&quot;, &quot;b&quot;] \u2502\n\u2502 2   \u2506 [&quot;a&quot;]      \u2502\n\u2502 3   \u2506 [&quot;c&quot;, &quot;d&quot;] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>I want to convert it to the following form.  I plan to save to a parquet file, and query the file (with sql).</p>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 &quot;a&quot; \u2502\n\u2502 1   \u2506 &quot;b&quot; \u2502\n\u2502 2   \u2506 &quot;a&quot; \u2502\n\u2502 3   \u2506 &quot;c&quot; \u2502\n\u2502 3   \u2506 &quot;d&quot; \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>I have seen an <a href=\"https://stackoverflow.com/questions/74638704/polars-unnesting-columns-algorithmically-without-a-for-loop\">answer that works on struct columns</a>, but <code>df.unnest('b')</code> on my data results in the error:</p>\n<pre><code>SchemaError: Series of dtype: List(Utf8) != Struct\n</code></pre>\n<p>I also found <a href=\"https://github.com/pola-rs/polars/issues/3282\" rel=\"noreferrer\">a github issue</a> that shows list can be converted to a struct, but I can't work out how to do that, or if it applies here.</p>\n"}, {"tags": ["python", "flask", "plotly-dash", "python-polars"], "owner": {"account_id": 34275955, "reputation": 129, "user_id": 26480677, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f7a9d0454c3f9319a889f9a5a6413791?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "PerovskiteCell", "link": "https://stackoverflow.com/users/26480677/perovskitecell"}, "is_answered": true, "view_count": 218, "answer_count": 2, "score": 10, "last_activity_date": 1724054471, "creation_date": 1722858642, "last_edit_date": 1722875496, "question_id": 78834305, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78834305/dash-polars-ram-use-keeps-increasing", "title": "Dash &amp; polars; RAM-use keeps increasing", "body": "<p>I have made a local dash-app to allow students to efficiently find/study their (measurement) data. To allow for further development, I tried to make a transition from pandas &amp; duckdb to polars. After weeks of work to integrate it into this extensive app, I realized that I have run into a major problem.</p>\n<p>The app was stable before, but now with polars, the RAM-footprint (of the pythonw.exe process) balloons with each successive callback. While the app starts out around 100 MB; each callback adds something like 5MB. I doesn\u2019t seem to stabilize; at 1500 MB it was still growing.</p>\n<p>I\u2019m sort of stuck and would really appreciate some pointers how to resolve this.</p>\n<p>I made a <strong>minimum example</strong> to illustrate the issue. If I run it with \u201cpolars_check=True\u201d, then I start with 98MB and after 100 iterations it has become 261 MB. If I do it with \u201cpolars_check\u201d=False (i.e. pandas), then I start and end with 98MB.</p>\n<pre><code>import pathlib, os, shutil\nimport polars as pl, pandas as pd, numpy as np, datetime as dt\n\nfrom dash import Dash, dcc, html, Input, Output\nimport plotly.graph_objects as go\n\n\n#Check-input\npolars_check = True ### Whether the example returns with polars or with pandas.\n\nif polars_check: #To accomdate the slower data retrieval with pandas.\n    interval_time = 3E3\nelse:\n    interval_time = 3E3\n\n#Constants\nfolder = pathlib.Path(r'C:\\PerovskiteCell example')\n\nn_files = 100 #Number of files in folder\nn_lines = 500000 #Number of total lines in folder\nn_cols = 25\n\n\n#Generating sample data in example folder (Only once).\nif not folder.exists():\n\n    size = int(n_lines / n_files)\n    col = np.linspace(-1E3, 1E3, num=size)\n\n    df = pl.DataFrame({f'col{n}': col for n in range(n_cols)})\n\n    # Creating folder &amp; files\n    os.makedirs(folder)\n\n    f_path0 = folder.joinpath('0.csv')\n    df.write_csv(f_path0)\n\n    for n in range(1, n_files):\n        shutil.copy2(f_path0, folder.joinpath(f'{n}.csv'))\n\n\n#Functions\ndef pl_data():\n    &quot;&quot;&quot;Retrieves data via the polars route&quot;&quot;&quot;\n    \n    lf = (pl.scan_csv(folder.joinpath(f'{n}.csv'),\n                                schema={f'col{n}': pl.Float64 for n in range(n_cols)})\n                            \n                            .select(pl.all().get(n)) for n in range(n_files))\n    \n    lf = pl.concat(lf)\n    lf = lf.select('col0', 'col1')\n\n    return lf.collect()\n\n\ndef pd_data():\n    &quot;&quot;&quot;Retrieves data via the pandas route&quot;&quot;&quot;\n\n    dfs = (pd.read_csv(folder.joinpath(f'{n}.csv'), usecols=['col0', 'col1']).iloc[n:n+1]\n                                         for n in range(n_files))\n    \n    return pd.concat(dfs, ignore_index=True)\n\n\n\n#App (initialization)\napp = Dash()\napp.layout = html.Div([dcc.Graph(id='graph'),\n                        dcc.Interval(id = 'check', \n                                        interval = interval_time,\n                                        max_intervals = 100)])\n\n\n@app.callback(\n    Output('graph', 'figure'),\n    Input('check', 'n_intervals'))\n\ndef plot(_):\n\n    #Data retrieval\n    if polars_check:\n        df = pl_data()\n    else:\n        df = pd_data()\n\n    #Plotting\n    fig = go.Figure()\n    trace = go.Scattergl(x = list(df['col0']), y=list(df['col1']), mode='lines+markers')\n\n    fig.add_trace(trace)\n    fig.update_xaxes(title = str(dt.datetime.now()))\n\n    return fig\n\n\nif __name__ == '__main__':\n    app.run(debug=False, port = 8050)\n</code></pre>\n"}, {"tags": ["python", "pandas", "python-polars"], "owner": {"user_type": "does_not_exist", "display_name": "user17260574"}, "is_answered": true, "view_count": 13193, "accepted_answer_id": 73703650, "answer_count": 6, "score": 9, "last_activity_date": 1714231124, "creation_date": 1663056283, "question_id": 73699500, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73699500/python-polars-split-string-column-into-many-columns-by-delimiter", "title": "python-polars split string column into many columns by delimiter", "body": "<p>In pandas, the following code will split the string from col1 into many columns. is there a way to do this in polars?</p>\n<pre><code>d = {'col1': [&quot;a/b/c/d&quot;, &quot;a/b/c/d&quot;]}\ndf= pd.DataFrame(data=d)\ndf[[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]]=df[&quot;col1&quot;].str.split('/',expand=True)\n</code></pre>\n"}, {"tags": ["dataframe", "python-polars"], "owner": {"user_type": "does_not_exist", "display_name": "user554319"}, "is_answered": true, "view_count": 11715, "accepted_answer_id": 72359324, "answer_count": 3, "score": 9, "last_activity_date": 1721474178, "creation_date": 1653378991, "question_id": 72359181, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72359181/how-to-select-columns-by-data-type-in-polars", "title": "How to select columns by data type in Polars?", "body": "<p>In pandas we have the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\" rel=\"noreferrer\"><code>pandas.DataFrame.select_dtypes</code></a> method that selects certain columns depending on the <code>dtype</code>. Is there a similar way to do such a thing in Polars?</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 23064686, "reputation": 95, "user_id": 17179289, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyqwf-2HvMQDn0cCupHeOPqOHuCDAP98UIBvVFy=k-s256", "display_name": "Rakesh Chaudhary", "link": "https://stackoverflow.com/users/17179289/rakesh-chaudhary"}, "is_answered": true, "view_count": 3914, "accepted_answer_id": 75994185, "answer_count": 2, "score": 9, "last_activity_date": 1685792660, "creation_date": 1681292949, "last_edit_date": 1681293733, "question_id": 75994078, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75994078/convert-2-columns-of-polars-dataframe-to-dictionary-having-its-key-as-first-colu", "title": "convert 2 columns of polars dataframe to dictionary having its key as first column elements and second column elements as values", "body": "<p>I am using below dataframe to convert to dictionary in specific format.</p>\n<p>However, I am getting an error TypeError: unhashable type: 'Series'</p>\n<pre><code>import polars as pl\n\n#input (polars eager dataframe):\npolar_df = pl.DataFrame(\n&quot;foo&quot;: ['a', 'b', 'c'],\n&quot;bar&quot;: [[6.0, 7.0, 8.0],[9.0,10.0,11.0],[12.0,13.0,14.0]]\n)\n\n#expected output (dictionary):\n#{'a':[6.0, 7.0, 8.0],'b':[9.0,10.0,11.0],'c':[12.0,13.0,14.0]}\n\ndict_output = \ndict(zip(polar_df.select(pl.col('foo')),\npolar_df.select(pl.col('bar'))\n))\n</code></pre>\n"}, {"tags": ["dataframe", "python-polars"], "owner": {"account_id": 1276881, "reputation": 427, "user_id": 1231940, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/56c08803b91bc71e7d3de76da5b73f2a?s=256&d=identicon&r=PG", "display_name": "Kaster", "link": "https://stackoverflow.com/users/1231940/kaster"}, "is_answered": true, "view_count": 9117, "accepted_answer_id": 74203380, "answer_count": 4, "score": 9, "last_activity_date": 1717146967, "creation_date": 1666760525, "last_edit_date": 1717146967, "question_id": 74202907, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74202907/polars-how-to-get-element-from-a-column-with-list-by-index-specified-in-anothe", "title": "(Polars) How to get element from a column with list by index specified in another column", "body": "<p>I have a dataframe with 2 columns, where first column contains lists, and second column integer indexes.</p>\n<p>How to get elements from first column by index specified in second column? Or even better, put that element in 3rd column.</p>\n<p><strong>Input example</strong></p>\n<pre><code>df = pl.DataFrame({\n    &quot;lst&quot;: [[1, 2, 3], [4, 5, 6]], \n    &quot;ind&quot;: [1, 2]\n})\n</code></pre>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 lst       \u2506 ind \u2502\n\u2502 ---       \u2506 --- \u2502\n\u2502 list[i64] \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [1, 2, 3] \u2506 1   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 [4, 5, 6] \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p><strong>Expected output.</strong></p>\n<pre><code>res = df.with_columns(pl.Series(&quot;list[ind]&quot;, [2, 6]))\n</code></pre>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 lst       \u2506 ind \u2506 list[ind] \u2502\n\u2502 ---       \u2506 --- \u2506 ---       \u2502\n\u2502 list[i64] \u2506 i64 \u2506 i64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [1, 2, 3] \u2506 1   \u2506 2         \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 [4, 5, 6] \u2506 2   \u2506 6         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>Thanks.</p>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 2527520, "reputation": 3036, "user_id": 2287458, "user_type": "registered", "accept_rate": 64, "profile_image": "https://i.sstatic.net/dAyBK.jpg?s=256", "display_name": "Phil-ZXX", "link": "https://stackoverflow.com/users/2287458/phil-zxx"}, "is_answered": true, "view_count": 462, "accepted_answer_id": 78822195, "answer_count": 2, "score": 9, "last_activity_date": 1722549749, "creation_date": 1722531380, "question_id": 78822168, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78822168/use-polars-when-then-otherwise-on-multiple-output-columns-at-once", "title": "Use polars when-then-otherwise on multiple output columns at once", "body": "<p>Assume I have this dataframe</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame({\n    'item':         ['CASH', 'CHECK', 'DEBT', 'CHECK', 'CREDIT', 'CASH'],\n    'quantity':     [100, -20, 0, 10, 0, 0],\n    'value':        [99, 47, None, 90, None, 120],\n    'value_other':  [97, 57, None, 91, None, 110],\n    'value_other2': [94, 37, None, 93, None, 115],\n})\n</code></pre>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 item   \u2506 quantity \u2506 value \u2506 value_other \u2506 value_other2 \u2502\n\u2502 ---    \u2506 ---      \u2506 ---   \u2506 ---         \u2506 ---          \u2502\n\u2502 str    \u2506 i64      \u2506 i64   \u2506 i64         \u2506 i64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CASH   \u2506 100      \u2506 99    \u2506 97          \u2506 94           \u2502\n\u2502 CHECK  \u2506 -20      \u2506 47    \u2506 57          \u2506 37           \u2502\n\u2502 DEBT   \u2506 0        \u2506 null  \u2506 null        \u2506 null         \u2502\n\u2502 CHECK  \u2506 10       \u2506 90    \u2506 91          \u2506 93           \u2502\n\u2502 CREDIT \u2506 0        \u2506 null  \u2506 null        \u2506 null         \u2502\n\u2502 CASH   \u2506 0        \u2506 120   \u2506 110         \u2506 115          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>Now I want to set all value columns to <code>0</code> for all rows where <code>value is null</code> and <code>quantity == 0</code>.</p>\n<p>Right now I have this solution</p>\n<pre class=\"lang-py prettyprint-override\"><code>cols = ['value', 'value_other', 'value_other2']\ndf   = df.with_columns([\n    pl.when(pl.col('value').is_null() &amp; (pl.col('quantity') == 0))\n    .then(0)\n    .otherwise(pl.col(col))\n    .alias(col)\n    for col in cols\n])\n</code></pre>\n<p>which correctly gives</p>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 item   \u2506 quantity \u2506 value \u2506 value_other \u2506 value_other2 \u2502\n\u2502 ---    \u2506 ---      \u2506 ---   \u2506 ---         \u2506 ---          \u2502\n\u2502 str    \u2506 i64      \u2506 i64   \u2506 i64         \u2506 i64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CASH   \u2506 100      \u2506 99    \u2506 97          \u2506 94           \u2502\n\u2502 CHECK  \u2506 -20      \u2506 47    \u2506 57          \u2506 37           \u2502\n\u2502 DEBT   \u2506 0        \u2506 0     \u2506 0           \u2506 0            \u2502\n\u2502 CHECK  \u2506 10       \u2506 90    \u2506 91          \u2506 93           \u2502\n\u2502 CREDIT \u2506 0        \u2506 0     \u2506 0           \u2506 0            \u2502\n\u2502 CASH   \u2506 0        \u2506 120   \u2506 110         \u2506 115          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>However, I feel this is very inefficient as my <code>when</code> condition is executed for every value column. Is there a way to achieve this using only polar internal functions &amp; without the native for-loop?</p>\n"}, {"tags": ["visual-studio-code", "jupyter", "python-polars"], "owner": {"account_id": 12616411, "reputation": 1148, "user_id": 9173710, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-6WrW0VtjLxA/AAAAAAAAAAI/AAAAAAAAAaw/kTh54_JVU1U/photo.jpg?sz=256", "display_name": "Raphael", "link": "https://stackoverflow.com/users/9173710/raphael"}, "is_answered": true, "view_count": 2780, "accepted_answer_id": 76432740, "answer_count": 1, "score": 9, "last_activity_date": 1722506364, "creation_date": 1673356571, "last_edit_date": 1722506364, "question_id": 75070559, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75070559/how-to-properly-display-a-polars-dataframe-in-vscode-jupyter-notebook-variables", "title": "How to properly display a Polars dataframe in VSCode Jupyter Notebook variables inspector", "body": "<p>Edit 2 (01.08.2024):<br />\nI believe VSCode has now moved onto the DataWrangler extension as their default data inspector and will deprecate the default one.<br />\n<a href=\"https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler\" rel=\"nofollow noreferrer\">https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler</a></p>\n<hr />\n<p>Edit:\nThis has been filed as bug in the Polars repository:\n<a href=\"https://github.com/pola-rs/polars/issues/6152\" rel=\"nofollow noreferrer\">https://github.com/pola-rs/polars/issues/6152</a><br />\nAnd the VSCode Jupyter repo:\n<a href=\"https://github.com/microsoft/vscode-jupyter/issues/12519\" rel=\"nofollow noreferrer\">https://github.com/microsoft/vscode-jupyter/issues/12519</a></p>\n<hr />\n<p>I am testing Python-Polars inside a Jupyter notebook in VSCode.</p>\n<p>When I open a data frame from the variable view, it is not formatted correctly.</p>\n<p>It shows like this:\n<a href=\"https://i.sstatic.net/maHwr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/maHwr.png\" alt=\"enter image description here\" /></a></p>\n<p>Columns and Rows are swapped and the column names are missing.</p>\n<p>I would've expected a display similar to pandas data frames like so:\n<a href=\"https://i.sstatic.net/uweG3.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/uweG3.png\" alt=\"enter image description here\" /></a></p>\n<p>How can I make the Polars dataframe display correctly?</p>\n"}, {"tags": ["parquet", "python-polars"], "owner": {"account_id": 3812270, "reputation": 5922, "user_id": 3162724, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/833457f6ec5ea62785cd84e0c312a8a4?s=256&d=identicon&r=PG", "display_name": "Niladri", "link": "https://stackoverflow.com/users/3162724/niladri"}, "is_answered": true, "view_count": 4040, "answer_count": 1, "score": 9, "last_activity_date": 1709379649, "creation_date": 1675184862, "last_edit_date": 1675237035, "question_id": 75300636, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75300636/issue-while-using-py-polars-sink-parquet-method-on-a-lazyframe", "title": "Issue while using py-polars sink_parquet method on a LazyFrame", "body": "<p>I am getting the below error while using sink_parquet on a LazyFrame. Earlier I was using <code>.collect()</code> on the output of the <code>scan_parquet()</code> to convert the result into a DataFrame but unfortunately it is not working with larger than RAM datasets. Here is the error I received -</p>\n<pre><code>PanicException: sink_parquet not yet supported in standard engine. Use 'collect().write_parquet()'\n</code></pre>\n<p>I am trying to write the LazyFrame (the output from scan_parquet) into a local file after I added some filter and join conditions on the LazyFrame.\nIt seems the error is coming from the below location -</p>\n<p><a href=\"https://github.com/pola-rs/polars/blob/master/py-polars/polars/internals/lazyframe/frame.py#L1235\" rel=\"noreferrer\">https://github.com/pola-rs/polars/blob/master/py-polars/polars/internals/lazyframe/frame.py#L1235</a> (In Python)</p>\n<p><a href=\"https://github.com/pola-rs/polars/blob/master/polars/polars-lazy/src/physical_plan/planner/lp.rs#L154\" rel=\"noreferrer\">https://github.com/pola-rs/polars/blob/master/polars/polars-lazy/src/physical_plan/planner/lp.rs#L154</a> (In Rust) .</p>\n<p>I have tried updating to the latest version <s>0.15.16</s> 0.16.1 but this issue still exists .</p>\n<p><strong>Sample code :</strong></p>\n<pre><code>pl.scan_parquet(&quot;path/to/file1.parquet&quot;)\n.select([\n    pl.col(&quot;col2&quot;),\n    pl.col(&quot;col2&quot;).apply( lambda x : ...)\n    .alias(&quot;splited_levels&quot;),\n    ..followed by more columns and .alias()\n])\n.join(&lt;another lazyframe&gt;,on=&quot;some key&quot;,how=&quot;inner&quot;)\n.filter(...)\n.filter(..)\n..followed by some more filters\n.sink_parquet(&quot;path/to/result2.parquet&quot;)\n</code></pre>\n<p>The parquet file should be written in local system. Instead I am getting the below error -</p>\n<pre><code>PanicException: sink_parquet not yet supported in standard engine. Use 'collect().write_parquet()'\n</code></pre>\n<p>Here are the details of the installed packages after I used <code>polars.show_versions()</code> -</p>\n<pre><code>--- Version info----\nPolars : 0.15.16\nIndex type : UInt32\nPlatform : Linux-4.15.0-191-generic-x86_64-with-glibc2.28\nPython: 3.9.16\n[GCC 8.3.0]\n--- Optional dependencies---\npyarrow : 11.0.0\npandas : not installed\nnumpy : 1.24.1\nfsspec : 2023.1.0\nconnectorx : not installed\nxlsx2csv : not installed\ndeltalake: not installed\nmatplotlib : not installed\n</code></pre>\n<p><strong>Update  :</strong> I have raised a github issue here for the same and it seems all types of queries are not supported for streaming at this moment . So I am looking for a work around in this case or any alternative way of doing this with polars\n<a href=\"https://github.com/pola-rs/polars/issues/6603\" rel=\"noreferrer\">https://github.com/pola-rs/polars/issues/6603</a></p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 11620322, "reputation": 1530, "user_id": 8511822, "user_type": "registered", "accept_rate": 77, "profile_image": "https://i.sstatic.net/h9Xmn.png?s=256", "display_name": "rchitect-of-info", "link": "https://stackoverflow.com/users/8511822/rchitect-of-info"}, "is_answered": true, "view_count": 13448, "accepted_answer_id": 71352644, "answer_count": 5, "score": 8, "last_activity_date": 1721474365, "creation_date": 1646396795, "question_id": 71351393, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71351393/polars-search-and-replace-in-column-names", "title": "Polars: Search and replace in column names", "body": "<p>This used to be handled in <code>pandas</code> as so:</p>\n<pre><code>df.columns = df.columns.str.replace('.','_')\n</code></pre>\n<p>This code works but definitely doesn't feel like the correct solution.</p>\n<pre><code>renamed = {}\nfor column_name in list(filter(lambda x: '.' in  x, df.columns)):\n    renamed[column_name] = column_name.replace('.', '_')\ndf = df.rename(renamed)\n</code></pre>\n<p>Thx</p>\n"}, {"tags": ["python", "pandas", "python-polars"], "owner": {"account_id": 4222868, "reputation": 1390, "user_id": 7340317, "user_type": "registered", "accept_rate": 100, "profile_image": "https://www.gravatar.com/avatar/6551ba6b36c4e4c6b45e3a6d7c11356d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Quant Christo", "link": "https://stackoverflow.com/users/7340317/quant-christo"}, "is_answered": true, "view_count": 3793, "accepted_answer_id": 74720969, "answer_count": 2, "score": 8, "last_activity_date": 1702550781, "creation_date": 1670431868, "question_id": 74720194, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74720194/polars-counting-elements-in-list-column", "title": "Polars counting elements in list column", "body": "<p>I've have dataframe with column <strong>b</strong> with list elements, I need to create column <strong>c</strong> that counts number elements in list for every row. Here is toy example in Pandas:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'a': [1,2,3], 'b':[[1,2,3], [2], [5,0]]})\n\n    a   b\n0   1   [1, 2, 3]\n1   2   [2]\n2   3   [5, 0]\n\ndf.assign(c=df['b'].str.len())\n\n    a   b           c\n0   1   [1, 2, 3]   3\n1   2   [2]         1\n2   3   [5, 0]      2\n\n</code></pre>\n<p>Here is my equivalent in Polars:</p>\n<pre><code>import polars as pl\n\ndfp = pl.DataFrame({'a': [1,2,3], 'b':[[1,2,3], [2], [5,0]]})\n\ndfp.with_columns(pl.col('b').apply(lambda x: len(x)).alias('c'))\n\n</code></pre>\n<p>I've a feeling that <code>.apply(lambda x: len(x))</code> is not optimal.</p>\n<p>Is a better way to do it in Polars?</p>\n"}, {"tags": ["python", "python-3.x", "dataframe", "python-polars", "rust-polars"], "owner": {"account_id": 7293211, "reputation": 726, "user_id": 5558953, "user_type": "registered", "accept_rate": 62, "profile_image": "https://www.gravatar.com/avatar/1b9be894a5421a105dfeee1747d729ed?s=256&d=identicon&r=PG", "display_name": "Dimitrius", "link": "https://stackoverflow.com/users/5558953/dimitrius"}, "is_answered": true, "view_count": 9043, "accepted_answer_id": 73744527, "answer_count": 2, "score": 8, "last_activity_date": 1717097648, "creation_date": 1663324087, "last_edit_date": 1663324425, "question_id": 73743437, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73743437/how-can-i-add-a-column-of-empty-arrays-to-polars-dataframe", "title": "How can I add a column of empty arrays to polars.DataFrame?", "body": "<p>I am trying to add a column of empty lists to a polars dataframe in python.</p>\n<p>My code</p>\n<pre class=\"lang-python prettyprint-override\"><code>import polars as pl\na = pl.DataFrame({'a': [1, 2, 3]})\na.with_columns([pl.lit([]).alias('b')])\n</code></pre>\n<p>throws</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;\n    a.with_columns([pl.lit([]).alias('b')])\n  File &quot;/usr/local/lib/python3.10/site-packages/polars/internals/lazy_functions.py&quot;, line 767, in lit\n    return pli.wrap_expr(pylit(item, allow_object))\nValueError: could not convert value '[]' as a Literal\n</code></pre>\n<p>How can I create this column?</p>\n"}, {"tags": ["python", "python-3.x", "pandas", "dataframe", "python-polars"], "owner": {"account_id": 17211710, "reputation": 103, "user_id": 21616488, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mBCbXFLv8s87yOFW8uYX4uNi0pzzFCSIvMfk3NH=k-s256", "display_name": "montol", "link": "https://stackoverflow.com/users/21616488/montol"}, "is_answered": true, "view_count": 4495, "accepted_answer_id": 77882017, "answer_count": 2, "score": 8, "last_activity_date": 1709619863, "creation_date": 1706204554, "last_edit_date": 1709619863, "question_id": 77881942, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77881942/whats-the-polars-equivalent-to-the-pandas-iloc-method", "title": "What&#39;s the polars equivalent to the pandas `.iloc` method?", "body": "<p>I'm looking for the recommended way to select an individual row of a <code>polars.DataFrame</code> by row number: something largely equivalent to <code>pandas.DataFrame</code>'s <code>.iloc[[n]]</code> method for a given integer <code>n</code>.</p>\n<p>For polars imported as <code>pl</code> and a polars DataFrame <code>df</code>, my current approach would be:</p>\n<pre class=\"lang-py prettyprint-override\"><code># for example\nn = 3\n\n# create row count, filter for individual row, drop the row count.\nnew_df = (\n    df.with_row_count()\n    .filter(pl.col('row_nr') == n)\n    .select(pl.exclude('row_nr'))\n)\n</code></pre>\n<p>I'm migrating from Pandas, and I've read the Pandas-to-Polars <a href=\"https://docs.pola.rs/user-guide/migration/pandas/\" rel=\"noreferrer\">migration guide</a>, but a slick solution to this specific case wasn't addressed there. <em>Edit: to clarify, I am looking for an approach that returns a <code>polars.DataFrame</code> object for the chosen row.</em></p>\n<p>Does anyone have something slicker?</p>\n"}, {"tags": ["python", "string", "replace", "python-polars"], "owner": {"account_id": 21102947, "reputation": 107, "user_id": 15512500, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8d3aba934b5e37b4a7ca05e8dc22f3d0?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Shamatix", "link": "https://stackoverflow.com/users/15512500/shamatix"}, "is_answered": true, "view_count": 6101, "accepted_answer_id": 73427426, "answer_count": 1, "score": 8, "last_activity_date": 1720303992, "creation_date": 1661002026, "last_edit_date": 1720303833, "question_id": 73427091, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73427091/polars-replace-part-of-string-in-column-with-value-of-other-column", "title": "Polars - Replace part of string in column with value of other column", "body": "<p>So I have a Polars dataframe looking as such</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame(\n    {\n        &quot;ItemId&quot;: [15148, 15148, 24957],\n        &quot;SuffixFactor&quot;: [19200, 200, 24],\n        &quot;ItemRand&quot;: [254, -1, -44],\n        &quot;Stat0&quot;: ['+5 Defense', '+$i Might', '+9 Vitality'],\n        &quot;Amount&quot;: ['', '7', '']\n    }\n)\n</code></pre>\n<p>I want to replace $i in the column &quot;Stat0&quot; with Amount whenever Stat0 contains i$</p>\n<p>I have tried a couple different things such as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.with_columns(\n    pl.col('Stat0').str.replace(r'\\$i', pl.col('Amount'))\n)\n</code></pre>\n<p>Expected result</p>\n<pre class=\"lang-py prettyprint-override\"><code>result = pl.DataFrame(\n    {\n        &quot;ItemId&quot;: [15148, 15148, 24957],\n        &quot;SuffixFactor&quot;: [19200, 200, 24],\n        &quot;ItemRand&quot;: [254, -1, -44],\n        &quot;Stat0&quot;: ['+5 Defense', '+7 Might', '+9 Vitality'],\n        &quot;Amount&quot;: ['', '7', '']\n    }\n)\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>shape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ItemId \u2506 SuffixFactor \u2506 ItemRand \u2506 Stat0       \u2506 Amount \u2502\n\u2502 ---    \u2506 ---          \u2506 ---      \u2506 ---         \u2506 ---    \u2502\n\u2502 i64    \u2506 i64          \u2506 i64      \u2506 str         \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 15148  \u2506 19200        \u2506 254      \u2506 +5 Defense  \u2506        \u2502\n\u2502 15148  \u2506 200          \u2506 -1       \u2506 +7 Might    \u2506 7      \u2502\n\u2502 24957  \u2506 24           \u2506 -44      \u2506 +9 Vitality \u2506        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>But this doesn't seem to work.</p>\n<p>I hope someone can help.</p>\n<p>Best regards</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 13081122, "reputation": 141, "user_id": 9451644, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-aMJ25ccbK7o/AAAAAAAAAAI/AAAAAAAAAFE/3kr-STXSPfQ/photo.jpg?sz=256", "display_name": "Muhammad D Vikar", "link": "https://stackoverflow.com/users/9451644/muhammad-d-vikar"}, "is_answered": true, "view_count": 2578, "accepted_answer_id": 75658737, "answer_count": 3, "score": 8, "last_activity_date": 1720531475, "creation_date": 1678161755, "question_id": 75657940, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75657940/how-custom-sort-of-rows-in-polars", "title": "how custom sort of rows in polars", "body": "<p>How to sort row with spesific order</p>\n<pre><code>df = pl.DataFrame({&quot;currency&quot;: [&quot;EUR&quot;,&quot;EUR&quot;,&quot;EUR&quot;,&quot;USD&quot;,&quot;USD&quot;,&quot;USD&quot;], &quot;alphabet&quot;: [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]})\n</code></pre>\n<p>i need to descending the currency and custom sort of alphabet</p>\n<p>expected to be like this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>currency</th>\n<th>alphabet</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>USD</td>\n<td>C</td>\n</tr>\n<tr>\n<td>USD</td>\n<td>A</td>\n</tr>\n<tr>\n<td>USD</td>\n<td>B</td>\n</tr>\n<tr>\n<td>EUR</td>\n<td>C</td>\n</tr>\n<tr>\n<td>EUR</td>\n<td>A</td>\n</tr>\n<tr>\n<td>EUR</td>\n<td>B</td>\n</tr>\n</tbody>\n</table>\n</div>"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 5152458, "reputation": 3173, "user_id": 4126652, "user_type": "registered", "accept_rate": 72, "profile_image": "https://www.gravatar.com/avatar/9f8d6d542cad4b8bfa7f2510804e8fbf?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Vikash Balasubramanian", "link": "https://stackoverflow.com/users/4126652/vikash-balasubramanian"}, "is_answered": true, "view_count": 5666, "accepted_answer_id": 72821688, "answer_count": 1, "score": 8, "last_activity_date": 1724424018, "creation_date": 1656618296, "last_edit_date": 1724424015, "question_id": 72821244, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72821244/polars-get-grouped-rows-where-column-value-is-maximum", "title": "Polars get grouped rows where column value is maximum", "body": "<p>So consider this snippet</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame({'class': ['a', 'a', 'b', 'b'], 'name': ['Ron', 'Jon', 'Don', 'Von'], 'score': [0.2, 0.5, 0.3, 0.4]})\ndf.group_by('class').agg(pl.col('score').max())\n</code></pre>\n<p>This gives me:</p>\n<pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 class \u2506 score \u2502\n\u2502 ---   \u2506 ---   \u2502\n\u2502 str   \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a     \u2506 0.5   \u2502\n\u2502 b     \u2506 0.4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>But I want the entire row of the group that corresponded to the maximum score. I can do a join with the original dataframe like</p>\n<pre><code>sdf = df.group_by('class').agg(pl.col('score').max())\nsdf.join(df, on=['class', 'score'])\n</code></pre>\n<p>To get</p>\n<pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 class \u2506 score \u2506 name \u2502\n\u2502 ---   \u2506 ---   \u2506 ---  \u2502\n\u2502 str   \u2506 f64   \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a     \u2506 0.5   \u2506 Jon  \u2502\n\u2502 b     \u2506 0.4   \u2506 Von  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>Is there any way to avoid the join and include the name column as part of the groupby aggregation?</p>\n"}, {"tags": ["python-3.x", "python-polars"], "owner": {"account_id": 5431499, "reputation": 147, "user_id": 17033672, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/03690b2ac11a216926a5cd9f7a21dff1?s=256&d=identicon&r=PG", "display_name": "user17033672", "link": "https://stackoverflow.com/users/17033672/user17033672"}, "is_answered": true, "view_count": 1350, "accepted_answer_id": 77441023, "answer_count": 3, "score": 8, "last_activity_date": 1717072075, "creation_date": 1699384573, "last_edit_date": 1699387073, "question_id": 77440877, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77440877/explode-out-polars-column-of-list-items-horizontally-to-new-columns", "title": "Explode out polars column of list items horizontally to new columns", "body": "<p>Is there a polars native way to explode out a column of lists horizontally?</p>\n<p>e.g. go from this:</p>\n<pre><code>df = pl.DataFrame(\n    {\n        &quot;letters&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n        &quot;numbers&quot;: [[1, 1], [2, 3], [4, 5], [6, 7]],\n    }\n)\n</code></pre>\n<p>To this?</p>\n<pre><code>df = pl.DataFrame(\n    {\n        &quot;letters&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n        &quot;numbers_a&quot;: [1, 2, 4, 6],\n        &quot;numbers_b&quot;: [1, 3, 5, 7]\n    }\n)\n</code></pre>\n<p>I am aware of the <code>.explode()</code> method but afaik this is only possible to use vertically</p>\n"}, {"tags": ["pandas", "dataframe", "python-polars"], "owner": {"account_id": 23280909, "reputation": 303, "user_id": 17364376, "user_type": "registered", "profile_image": "https://i.sstatic.net/amFXz.jpg?s=256", "display_name": "Rahil", "link": "https://stackoverflow.com/users/17364376/rahil"}, "is_answered": true, "view_count": 41519, "accepted_answer_id": 71669042, "answer_count": 2, "score": 8, "last_activity_date": 1654604525, "creation_date": 1648536274, "question_id": 71657556, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71657556/error-while-converting-pandas-dataframe-to-polars-dataframe-pyarrow-lib-arrowty", "title": "Error while converting pandas dataframe to polars dataframe (pyarrow.lib.ArrowTypeError: Expected bytes, got a &#39;int&#39; object)", "body": "<p>I am converting pandas dataframe to polars dataframe but pyarrow throws error.</p>\n<p>My code:</p>\n<pre><code>import polars as pl\nimport pandas as pd\n\nif __name__ == &quot;__main__&quot;:\n\n    with open(r&quot;test.xlsx&quot;, &quot;rb&quot;) as f:\n        excelfile = f.read()\n    excelfile = pd.ExcelFile(excelfile)\n    sheetnames = excelfile.sheet_names\n    df = pd.concat(\n        [\n            pd.read_excel(\n            excelfile, sheet_name=x, header=0)\n                    for x in sheetnames\n                    ], axis=0)\n\n    df_pl = pl.from_pandas(df)\n</code></pre>\n<p>Error:</p>\n<p><code>File &quot;pyarrow\\array.pxi&quot;, line 312, in pyarrow.lib.array </code></p>\n<p><code>File &quot;pyarrow\\array.pxi&quot;, line 83, in pyarrow.lib._ndarray_to_array </code></p>\n<p><code>File &quot;pyarrow\\error.pxi&quot;, line 122, in pyarrow.lib.check_status </code></p>\n<p><code>pyarrow.lib.ArrowTypeError: Expected bytes, got a 'int' object </code></p>\n<p>I tried changing pandas dataframe <code>dtype</code> to <code>str</code> and problem is solved, but i don't want to change <code>dtypes</code>. Is it bug in pyarrow or am I missing something?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 19666406, "reputation": 81, "user_id": 14396723, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/320dcd511827154acd6a28cc124c8c21?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "noobanalyst", "link": "https://stackoverflow.com/users/14396723/noobanalyst"}, "is_answered": true, "view_count": 780, "protected_date": 1708969748, "answer_count": 2, "score": 8, "last_activity_date": 1708970650, "creation_date": 1708969402, "last_edit_date": 1708970538, "question_id": 78062891, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78062891/polars-how-to-use-pl-when-to-output-a-string-value", "title": "Polars - How to use `pl.when` to output a string value", "body": "<p>I am trying to use polars' <a href=\"https://docs.pola.rs/py-polars/html/reference/expressions/api/polars.when.html\" rel=\"noreferrer\"><code>pl.when</code></a> to create a new string column.  Based on a condition, the column should contain <code>&quot;string a&quot;</code> or <code>&quot;string b&quot;</code>.</p>\n<p>Polars seems to interpret <code>&quot;string a&quot;</code> as a column, when I try the following.</p>\n<pre><code>pl.when(condition)\n.then('string a')\n.otherwise('string b')\n.alias('new column')\n</code></pre>\n<p><strong>Output.</strong></p>\n<pre><code>ColumnNotFoundError: string a\n</code></pre>\n<p>I am expecting an output like this:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th>new column</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>string a</td>\n</tr>\n<tr>\n<td>string b</td>\n</tr>\n</tbody>\n</table></div>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 19765474, "reputation": 319, "user_id": 14473607, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiwiEq2qAWTL55Xr_IsvBSi2_ccRO_CljEj3rtONw=k-s256", "display_name": "Maiia Bocharova", "link": "https://stackoverflow.com/users/14473607/maiia-bocharova"}, "is_answered": true, "view_count": 12372, "closed_date": 1668544999, "accepted_answer_id": 74438454, "answer_count": 1, "score": 7, "last_activity_date": 1706280441, "creation_date": 1668438990, "question_id": 74433918, "link": "https://stackoverflow.com/questions/74433918/apply-a-function-to-2-columns-in-polars", "closed_reason": "Duplicate", "title": "Apply a function to 2 columns in Polars", "body": "<p>I want to apply a custom function which takes 2 columns and outputs a value based on those (row-based)</p>\n<p>In Pandas there is a syntax to apply a function based on values in multiple columns</p>\n<pre><code>df['col_3'] = df.apply(lambda x: func(x.col_1, x.col_2), axis=1)\n</code></pre>\n<p>What is the syntax for this in Polars?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 1004826, "reputation": 2952, "user_id": 1018861, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/e4df1317aa64a5f6550ff47202bbc5a0?s=256&d=identicon&r=PG", "display_name": "TomNorway", "link": "https://stackoverflow.com/users/1018861/tomnorway"}, "is_answered": true, "view_count": 7451, "accepted_answer_id": 76626203, "answer_count": 3, "score": 7, "last_activity_date": 1714726726, "creation_date": 1657357858, "last_edit_date": 1671813125, "question_id": 72920189, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72920189/select-all-columns-where-column-name-starts-with-string", "title": "Select all columns where column name starts with string", "body": "<p>Given the following dataframe, is there some way to select only columns starting with a given prefix? I know I could do e.g. <code>pl.col(column) for column in df.columns if column.startswith(&quot;prefix_&quot;)</code>, but I'm wondering if I can do it as part of a single expression.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame(\n    {&quot;prefix_a&quot;: [1, 2, 3], &quot;prefix_b&quot;: [1, 2, 3], &quot;some_column&quot;: [3, 2, 1]}\n)\ndf.select(pl.all().&lt;column_name_starts_with&gt;(&quot;prefix_&quot;))\n</code></pre>\n<p>Would this be possible to do lazily?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 4414299, "reputation": 3875, "user_id": 3596337, "user_type": "registered", "accept_rate": 62, "profile_image": "https://www.gravatar.com/avatar/e7077a19bac20bf5bc1ffb6e2c71039a?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "keiv.fly", "link": "https://stackoverflow.com/users/3596337/keiv-fly"}, "is_answered": true, "view_count": 13790, "accepted_answer_id": 71199958, "answer_count": 3, "score": 7, "last_activity_date": 1674529204, "creation_date": 1645376700, "question_id": 71196737, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71196737/how-to-filter-a-polars-dataframe-by-date", "title": "How to filter a polars dataframe by date?", "body": "<pre><code>df.filter(pl.col(&quot;MyDate&quot;) &gt;= &quot;2020-01-01&quot;)\n</code></pre>\n<p>does not work like it does in pandas.</p>\n<p>I found a workaround</p>\n<pre><code>df.filter(pl.col(&quot;MyDate&quot;) &gt;= pl.datetime(2020,1,1))\n</code></pre>\n<p>but this does not solve a problem if I need to use string variables.</p>\n"}, {"tags": ["python", "dataframe", "series", "python-polars"], "owner": {"account_id": 11553130, "reputation": 83, "user_id": 8465066, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9305fcab7850b8599c64928bb94ca71e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "SKJ", "link": "https://stackoverflow.com/users/8465066/skj"}, "is_answered": true, "view_count": 19225, "accepted_answer_id": 75528006, "answer_count": 2, "score": 7, "last_activity_date": 1723352939, "creation_date": 1677009463, "last_edit_date": 1723352939, "question_id": 75525312, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75525312/how-can-i-convert-a-polars-dataframe-to-a-python-list", "title": "How can I convert a Polars Dataframe to a Python list", "body": "<p>I understand Polars Series can be exported to a Python list. However, is there any way I can convert a Polars Dataframe to a Python list?</p>\n<p>In addition, if there is a one single column in the Polars Dataframe, how can I convert that into a Polars Series?</p>\n<p>I tried to use the pandas commands but it didn't work. I also checked the official Polars website to see any related built-in functions, but I didn't see any.</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 25601014, "reputation": 91, "user_id": 19376029, "user_type": "registered", "profile_image": "https://i.sstatic.net/FTepm.jpg?s=256", "display_name": "Thomas Lutterbeck", "link": "https://stackoverflow.com/users/19376029/thomas-lutterbeck"}, "is_answered": true, "view_count": 2972, "accepted_answer_id": 74481577, "answer_count": 4, "score": 7, "last_activity_date": 1721324006, "creation_date": 1655738016, "last_edit_date": 1721322710, "question_id": 72689281, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72689281/index-operation-on-list-column-data-in-polars", "title": "Index operation on list column data in polars", "body": "<p>I have a column with a list of Strings for which I need to check if a particular String occurs before another.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.LazyFrame(\n    {\n        'str': ['A', 'B', 'C', 'B', 'A'],\n        'group': [1,1,2,1,2]\n    }\n)\n\ndf_groups = df.group_by('group').agg(pl.col('str').alias('str_list'))\nprint(df_groups.collect())\n</code></pre>\n<pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 str_list        \u2502\n\u2502 ---   \u2506 ---             \u2502\n\u2502 i64   \u2506 list[str]       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 [&quot;A&quot;, &quot;B&quot;, &quot;B&quot;] \u2502\n\u2502 2     \u2506 [&quot;C&quot;, &quot;A&quot;]      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>I have created the following code example that works, but needs to break out of polars using <code>map_elements</code>, which makes it very slow.</p>\n<pre class=\"lang-py prettyprint-override\"><code>pre = 'A'\nsucc = 'B'\n\ndf_groups_filtered = df_groups.filter(\n    pl.col('str_list').map_elements(\n        lambda str_list: \n            pre in str_list and succ in str_list and \n            str_list.to_list().index(pre) &lt; str_list.to_list().index(succ)\n    )\n)\n\ndf_groups_filtered.collect()\n</code></pre>\n<p>This provides the desired result:</p>\n<pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 str_list        \u2502\n\u2502 ---   \u2506 ---             \u2502\n\u2502 i64   \u2506 list[str]       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 [&quot;A&quot;, &quot;B&quot;, &quot;B&quot;] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>I know that I can do</p>\n<pre class=\"lang-py prettyprint-override\"><code>df_groups_filtered = df_groups.filter(\n    pl.col('str_list').list.contains(pre) &amp; col('str_list').list.contains(succ)\n)\n</code></pre>\n<p>for the part of checking that both strings are contained, but I couldn't figure out how I can check the order in pure polars.</p>\n<p>Are there ways to achieve this natively with polars?</p>\n"}, {"tags": ["python", "pandas", "python-polars"], "owner": {"account_id": 18552471, "reputation": 403, "user_id": 13518426, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/01984ebe104b04aacbcc27e2ec5cf426?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Ahmad", "link": "https://stackoverflow.com/users/13518426/ahmad"}, "is_answered": true, "view_count": 9059, "accepted_answer_id": 76076401, "answer_count": 3, "score": 7, "last_activity_date": 1722554792, "creation_date": 1680613421, "question_id": 75929721, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75929721/how-to-show-full-column-width-of-polars-dataframe-in-python", "title": "How to show full column width of polars dataframe in Python?", "body": "<p>I'm trying to display the full width of column in polars dataframe. Given the following polars dataframe:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl \n\ndf = pl.DataFrame({\n    'column_1': ['TF-IDF embeddings are done on the initial corpus, with no additional N-Gram representations or further preprocessing', 'In the eager API, the expression is evaluated immediately. The eager API produces results immediately after execution, similar to pandas. The lazy API is similar to Spark, where a plan is formed upon execution of a query, but the plan does not actually access the data until the collect method is called to execute the query in parallel across all CPU cores. In simple terms: Lazy execution means that an expression is not immediately evaluated.'],\n    'column_2': ['Document clusterings may misrepresent the visualization of document clusterings due to dimensionality reduction (visualization is pleasing for its own sake - rather than for prediction/inference)', 'Polars has two APIs, eager and lazy. In the eager API, the expression is evaluated immediately. The eager API produces results immediately after execution, similar to pandas. The lazy API is similar to Spark, where a plan is formed upon execution of a query, but the plan does not actually access the data until the collect method is called to execute the query in parallel across all CPU cores. In simple terms: Lazy execution means that an expression is not immediately evaluated.']\n})\n</code></pre>\n<p>I tried the following:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pl.Config.set_fmt_str_lengths = 200\npl.Config.set_tbl_width_chars = 200\n</code></pre>\n<p>The result:</p>\n<pre class=\"lang-py prettyprint-override\"><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_1                          \u2506 column_2                          \u2502\n\u2502 ---                               \u2506 ---                               \u2502\n\u2502 str                               \u2506 str                               \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 TF-IDF embeddings are done on th\u2026 \u2506 Document clusterings may misrepr\u2026 \u2502\n\u2502 In the eager API, the expression\u2026 \u2506 Polars has two APIs, eager and l\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>How can I display the full width of columns in a polars DataFrame in Python?</p>\n<p>Thanks in advance!</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 3445969, "reputation": 389, "user_id": 2886789, "user_type": "registered", "accept_rate": 70, "profile_image": "https://www.gravatar.com/avatar/cd1ff102dbe9d54137e60c97f9e23f51?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "daeda", "link": "https://stackoverflow.com/users/2886789/daeda"}, "is_answered": true, "view_count": 12609, "accepted_answer_id": 71759376, "answer_count": 1, "score": 7, "last_activity_date": 1724207446, "creation_date": 1649192031, "last_edit_date": 1724207446, "question_id": 71758283, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71758283/how-is-python-polars-treating-the-index", "title": "How is Python Polars treating the index?", "body": "<p>I want to try out polars in Python so what I want to do is concatenate several dataframes that are read from jsons. When I change the index to <code>date</code> and have a look at <code>lala1.head()</code> I see that the column <code>date</code> is gone, so I basically lose the index. Is there a better solution or do I need to sort by date, which basically does the same as setting the index to <code>date</code>?</p>\n<pre><code>import polars as pl\n\nquarterly_balance_df = pl.read_json('../AAPL/single_statements/1985-09-30-quarterly_balance.json')\n\n\nq1 = quarterly_balance_df.lazy().with_column(pl.col(&quot;date&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))\nquarterly_balance_df = q1.collect()\nq2 = quarterly_balance_df.lazy().with_column(pl.col(&quot;fillingDate&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))\nquarterly_balance_df = q2.collect()\nq3 = quarterly_balance_df.lazy().with_column(pl.col(&quot;acceptedDate&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))\nquarterly_balance_df = q3.collect()\n\nquarterly_balance_df2 = pl.read_json('../AAPL/single_statements/1986-09-30-quarterly_balance.json')\n\nq1 = quarterly_balance_df2.lazy().with_column(pl.col(&quot;date&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))\nquarterly_balance_df2 = q1.collect()\nq2 = quarterly_balance_df2.lazy().with_column(pl.col(&quot;fillingDate&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))\nquarterly_balance_df2 = q2.collect()\nq3 = quarterly_balance_df2.lazy().with_column(pl.col(&quot;acceptedDate&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))\nquarterly_balance_df2 = q3.collect()\n\nlala1 = pl.from_pandas(quarterly_balance_df.to_pandas().set_index('date'))\nlala2 = pl.from_pandas(quarterly_balance_df.to_pandas().set_index('date'))\n\ntest = pl.concat([lala1,lala2])\n</code></pre>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 13972287, "reputation": 774, "user_id": 10090558, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8e9c7ffd9240f82201b3a4b56b2889ba?s=256&d=identicon&r=PG", "display_name": "bumpbump", "link": "https://stackoverflow.com/users/10090558/bumpbump"}, "is_answered": true, "view_count": 4619, "accepted_answer_id": 71185437, "answer_count": 1, "score": 7, "last_activity_date": 1645277163, "creation_date": 1645214866, "question_id": 71179317, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71179317/how-to-limit-number-of-threads-in-polars", "title": "how to limit number of threads in polars", "body": "<p>Is there a way to limit the number of threads used by polars?</p>\n<p>I am doing this because I am doing a second layer of parallelization around some polars code, and would like to limit the inner parallelism. This should still be better than Pandas due to the SIMD right.</p>\n"}, {"tags": ["python", "datetime", "python-polars", "strftime"], "owner": {"account_id": 21289179, "reputation": 687, "user_id": 20591261, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GihXW0TSZEFOqoXIUVuaiLTwptOiR0yYgXOOkJ7oA=k-s256", "display_name": "Simon", "link": "https://stackoverflow.com/users/20591261/simon"}, "is_answered": true, "view_count": 133, "accepted_answer_id": 78841114, "answer_count": 2, "score": 7, "last_activity_date": 1723020199, "creation_date": 1722978550, "last_edit_date": 1723020199, "question_id": 78841010, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78841010/format-datetime-in-polars", "title": "Format datetime in polars", "body": "<p>I have a polars dataframe that contains a <code>datetime</code> column. I want to convert this column to strings in the format <code>%Y%m</code>. For example, all dates in January 2024 should be converted to <code>&quot;202401&quot;</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport polars as pl\n\n\ndata = {\n    &quot;ID&quot; : [1,2,3],\n    &quot;dates&quot; : [datetime(2024,1,2),datetime(2024,1,3),datetime(2024,1,4)],\n}\n\ndf = pl.DataFrame(data)\n</code></pre>\n<p>I have tried using <code>strftime</code>. However, the following <code>AttributeError</code> is raised.</p>\n<pre class=\"lang-py prettyprint-override\"><code>AttributeError: 'Expr' object has no attribute 'strftime'\n</code></pre>\n"}, {"tags": ["dataframe", "python-polars"], "owner": {"account_id": 27271873, "reputation": 363, "user_id": 20793070, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp4BYW3zQ-viqwM10P8A3sCffwZwDaSw4fTTloHH=k-s256", "display_name": "Jahspear", "link": "https://stackoverflow.com/users/20793070/jahspear"}, "is_answered": true, "view_count": 4083, "accepted_answer_id": 74867502, "answer_count": 1, "score": 7, "last_activity_date": 1721336688, "creation_date": 1671560184, "last_edit_date": 1721336688, "question_id": 74867297, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74867297/properly-groupby-and-filter-with-polars", "title": "Properly groupby and filter with Polars", "body": "<p>I have df for my work with 3 main columns: <code>cid1, cid2, cid3</code>, and more 7 columns <code>cid4, cid5, etc</code>.</p>\n<p><code>cid1</code> and <code>cid2</code> is <code>int</code>, another columns is <code>float</code>.</p>\n<p>Each combitations of <code>cid1</code> and <code>cid2</code> is a workset with some rows where is values of all other columns is different. I want to filter df and receive my df with only max values in column <code>cid3</code> for each combination of <code>cid1</code> and <code>cid2</code>. <code>cid4</code> and next columns must be leaved without changes.</p>\n<p>This code helps me with one part of my task:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = (df\n    .group_by(&quot;cid1&quot;, &quot;cid2&quot;)\n    .agg(pl.max(&quot;cid3&quot;).alias(&quot;max_cid3&quot;))\n)\n</code></pre>\n<p>It's receives only 3 columns: <code>cid1</code>, <code>cid2</code>, <code>max_cid3</code> and filter all rows when <code>cid3</code> is not maximal.\nBut I can't find how to receive all another columns (<code>cid4, etc</code>) for that rows without changes.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = (df\n    .group_by(&quot;cid1&quot;, &quot;cid2&quot;)\n    .agg(pl.max(&quot;cid3&quot;).alias(&quot;max_cid3&quot;), pl.col(&quot;cid4&quot;))\n)\n</code></pre>\n<p>I tried to add <code>pl.col(&quot;cid4&quot;)</code> to list of aggs but in column I see as values different lists of some <code>cid4</code> values.</p>\n<p>How I can make it properly? Maybe Polars haves another way to make it then group_by?</p>\n<p>In Pandas I can make it:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf[&quot;max_cid3&quot;] = df.groupby(['cid1', 'cid2'])['cid3'].transform(np.max)\n</code></pre>\n<p>And then filter df wherever <code>cid3==max_cid3</code>\nBut I can't find a way to make it in Polars.</p>\n<p>Thank you!</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 23002595, "reputation": 143, "user_id": 17126369, "user_type": "registered", "profile_image": "https://i.sstatic.net/8gIUl.jpg?s=256", "display_name": "PierXuY", "link": "https://stackoverflow.com/users/17126369/pierxuy"}, "is_answered": true, "view_count": 375, "accepted_answer_id": 78869581, "answer_count": 2, "score": 7, "last_activity_date": 1723620538, "creation_date": 1723604482, "last_edit_date": 1723605070, "question_id": 78868961, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78868961/how-to-get-the-value-of-a-specified-index-number-from-the-sorting-of-a-column-an", "title": "How to get the value of a specified index number from the sorting of a column and fill it with null if missing?", "body": "<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame(\n    {&quot;name&quot;: list(&quot;abcdef&quot;), &quot;age&quot;: [21, 31, 32, 53, 45, 26], &quot;country&quot;: list(&quot;AABBBC&quot;)}\n)\n\ndf.group_by(&quot;country&quot;).agg(\n    pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).first().alias(&quot;age_sort_1&quot;),\n    pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).get(2).alias(&quot;age_sort_2&quot;),  # OutOfBoundsError: index out of bounds\n    # pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).arr.get(2, null_on_oob=True).alias(&quot;age_2&quot;),\n    # SchemaError: invalid series dtype: expected `FixedSizeList`, got `str`\n    pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).last().alias(&quot;age_sort_-1&quot;)\n)\n</code></pre>\n<p>As shown in the code above, I want to get the name in each country whose age is in a specific order.</p>\n<p>However, <code>Expr.get</code> does not provide the null_on_oob parameter. How to automatically fill in null when an out-of-bounds situation occurs?</p>\n<p>In addition, the <code>.arr.get</code> method provides the <code>null_on_oob parameter</code>, but reports an error <code>SchemaError: invalid series dtype: expected &quot;FixedSizeList&quot;, got &quot;str&quot;.</code> I don\u2019t know what this error refers to and how to solve it.</p>\n<p>ps: The above code uses the repeated code <code>pl.col(&quot;name&quot;).sort_by(&quot;age&quot;)</code> many times. Is there a more concise method?</p>\n"}, {"tags": ["python", "sqlite", "python-polars"], "owner": {"account_id": 14846440, "reputation": 1508, "user_id": 10721627, "user_type": "registered", "profile_image": "https://i.sstatic.net/rvGJC.jpg?s=256", "display_name": "P&#233;ter Szilv&#225;si", "link": "https://stackoverflow.com/users/10721627/p%c3%a9ter-szilv%c3%a1si"}, "is_answered": true, "view_count": 803, "accepted_answer_id": 78076008, "answer_count": 2, "score": 7, "last_activity_date": 1709152901, "creation_date": 1709134676, "last_edit_date": 1709152901, "question_id": 78075720, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78075720/how-to-write-to-a-sqlite-database-using-polars-in-python", "title": "How to write to a SQLite database using Polars in Python?", "body": "<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\nimport sqlite3\n\nconn = sqlite3.connect(&quot;test.db&quot;)\ndf = pl.DataFrame({&quot;col1&quot;: [1, 2, 3]})\n</code></pre>\n<p>According to the documentation of <a href=\"https://docs.pola.rs/py-polars/html/reference/api/polars.DataFrame.write_database.html\" rel=\"nofollow noreferrer\"><code>pl.write_database</code></a>, I need to pass a connection URI string e.g. &quot;sqlite:////path/to/database.db&quot; for SQLite database:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.write_database(&quot;test_table&quot;, f&quot;sqlite:////test.db&quot;, if_table_exists=&quot;replace&quot;)\n</code></pre>\n<p>However, I got the following error:</p>\n<pre><code>OperationalError: (sqlite3.OperationalError) unable to open database file\n</code></pre>\n<p>EDIT: Based on the answer, install SQLAlchemy with the <code>pip install polars[sqlalchemy]</code> command.</p>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 11848404, "reputation": 3184, "user_id": 9534390, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/371f1ed48e9e51e946b84cf5d17d366e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "pythonic833", "link": "https://stackoverflow.com/users/9534390/pythonic833"}, "is_answered": true, "view_count": 3534, "accepted_answer_id": 69591265, "answer_count": 2, "score": 7, "last_activity_date": 1676306014, "creation_date": 1634337083, "last_edit_date": 1634338248, "question_id": 69591233, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69591233/how-to-select-rows-between-a-certain-date-range-in-python-polars", "title": "How to select rows between a certain date range in python-polars?", "body": "<p>If a DataFrame is constructed like the following using polars-python:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\nfrom polars import col\nfrom datetime import datetime\n\ndf = pl.DataFrame({\n    &quot;dates&quot;: [&quot;2016-07-02&quot;, &quot;2016-08-10&quot;,  &quot;2016-08-31&quot;, &quot;2016-09-10&quot;],\n    &quot;values&quot;: [1, 2, 3, 4]\n})\n</code></pre>\n<p>How to select the rows between a certain date range, i.e. between between <code>&quot;2016-08-10&quot;</code> and <code>&quot;2016-08-31&quot;</code>, so that the desired outcome is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dates      \u2506 values \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2016-08-10 \u2506 2      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2016-08-31 \u2506 3      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n"}, {"tags": ["python", "pandas", "dataframe", "group-by", "python-polars"], "owner": {"account_id": 8153662, "reputation": 711, "user_id": 7775166, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/eece4c37ff32799e0e92f3aa36503c9b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "girdeux", "link": "https://stackoverflow.com/users/7775166/girdeux"}, "is_answered": true, "view_count": 145, "accepted_answer_id": 78726017, "answer_count": 1, "score": 7, "last_activity_date": 1720533131, "creation_date": 1720532568, "last_edit_date": 1720532970, "question_id": 78725967, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/78725967/translate-pandas-groupby-plus-resample-to-polars-in-python", "title": "Translate Pandas groupby plus resample to Polars in Python", "body": "<p>I have this code that generates a toy DataFrame (production df is much complex):</p>\n<pre><code>import polars as pl\nimport numpy as np\nimport pandas as pd\n\ndef create_timeseries_df(num_rows):\n    date_rng = pd.date_range(start='1/1/2020', end='1/01/2021', freq='T')\n    data = {\n        'date': np.random.choice(date_rng, num_rows),\n        'category': np.random.choice(['A', 'B', 'C', 'D'], num_rows),\n        'subcategory': np.random.choice(['X', 'Y', 'Z'], num_rows),\n        'value': np.random.rand(num_rows) * 100\n    }\n    df = pd.DataFrame(data)\n    df = df.sort_values('date')\n    df.set_index('date', inplace=True, drop=False)\n    df.index = pd.to_datetime(df.index)\n\n    return df\n\nnum_rows = 1000000  # for example\ndf = create_timeseries_df(num_rows)\n</code></pre>\n<p>Then perform this transformations with Pandas.</p>\n<pre><code>df_pd = df.copy()\ndf_pd = df_pd.groupby(['category', 'subcategory'])\ndf_pd = df_pd.resample('W-MON')\ndf_pd.agg({\n    'value': ['sum', 'mean', 'max', 'min']\n}).reset_index()\n</code></pre>\n<p>But, obviously it is quite slow with Pandas (at least in production). Thus, I'd like to use Polars to speed up time. This is what I have so far:</p>\n<pre><code>#Convert to Polars DataFrame\ndf_pl = pl.from_pandas(df)\n\n#Groupby, resample and aggregate\ndf_pl = df_pl.group_by(['category', 'subcategory'])\ndf_pl = df_pl.group_by_dynamic('date', every='1w', closed='right')\ndf_pl.agg([\n   pl.col('value').sum().alias('value_sum'),\n   pl.col('value').mean().alias('value_mean'),\n   pl.col('value').max().alias('value_max'),\n   pl.col('value').min().alias('value_min')\n])\n</code></pre>\n<p>But I get <code>AttributeError: 'GroupBy' object has no attribute 'group_by_dynamic'</code>. Any ideas on how to use <code>groupby</code> followed by <code>resample</code> in Polars?</p>\n"}, {"tags": ["pycharm", "jetbrains-ide", "python-polars"], "owner": {"account_id": 22121493, "reputation": 337, "user_id": 16374636, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0e4752e2d21c5459d396cc0fd9f57369?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "zacko", "link": "https://stackoverflow.com/users/16374636/zacko"}, "is_answered": true, "view_count": 2462, "accepted_answer_id": 72653095, "answer_count": 2, "score": 7, "last_activity_date": 1673107848, "creation_date": 1655373589, "last_edit_date": 1655382113, "question_id": 72644013, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72644013/how-to-show-polars-dataframe-in-pycharm", "title": "How to show Polars Dataframe in PyCharm", "body": "<p>In PyCharm you have the ability to show a Pandas Dataframe with the SciView tool. Is this also possible with Polars or would I have to spam <code>print</code> statements?</p>\n<p>(I also opened a PyCharm support ticket)</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 12185931, "reputation": 553, "user_id": 8895744, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4c435ce09e7de502d71a87b139720b4e?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "EnesZ", "link": "https://stackoverflow.com/users/8895744/enesz"}, "is_answered": true, "view_count": 3439, "accepted_answer_id": 75548927, "answer_count": 4, "score": 7, "last_activity_date": 1694618421, "creation_date": 1677174193, "question_id": 75548444, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75548444/polars-dataframe-drop-nans", "title": "Polars dataframe drop nans", "body": "<p>I need to drop rows that have a nan value in any column. As for null values with <code>drop_nulls()</code></p>\n<pre><code>df.drop_nulls()\n</code></pre>\n<p>but for nans. I have found that the method <code>drop_nans</code> exist for Series but not for DataFrames</p>\n<pre><code>df['A'].drop_nans()\n</code></pre>\n<p>Pandas code that I'm using:</p>\n<pre><code>df = pd.DataFrame(\n    {\n        'A': [0, 0, 0, 1,None, 1],\n        'B': [1, 2, 2, 1,1, np.nan]\n    }\n)\ndf.dropna()\n</code></pre>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 5573069, "reputation": 992, "user_id": 4417586, "user_type": "registered", "accept_rate": 75, "profile_image": "https://www.gravatar.com/avatar/c12a0f381503152d555f45886bf217da?s=256&d=identicon&r=PG", "display_name": "bolino", "link": "https://stackoverflow.com/users/4417586/bolino"}, "is_answered": true, "view_count": 9453, "accepted_answer_id": 70106847, "answer_count": 2, "score": 7, "last_activity_date": 1706688100, "creation_date": 1637813881, "question_id": 70105756, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70105756/what-is-the-polars-equivalent-of-pandas-isna-method", "title": "What is the Polars equivalent of Pandas `.isna()` method?", "body": "<p>I'm trying to replace Pandas with <a href=\"https://github.com/pola-rs/polars\" rel=\"noreferrer\">Polars</a> in production code, for better memory performance.</p>\n<p>What would be the Polars equivalent of Pandas <code>.isna()</code> method? I couldn't find any good equivalent in the doc.</p>\n"}, {"tags": ["python", "format", "python-polars", "f-string"], "owner": {"account_id": 14115577, "reputation": 24980, "user_id": 10197418, "user_type": "registered", "profile_image": "https://i.sstatic.net/be8Y6.jpg?s=256", "display_name": "FObersteiner", "link": "https://stackoverflow.com/users/10197418/fobersteiner"}, "is_answered": true, "view_count": 2671, "answer_count": 2, "score": 7, "last_activity_date": 1680239349, "creation_date": 1680161852, "last_edit_date": 1680168778, "question_id": 75885359, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75885359/how-to-convert-float-to-string-with-specific-number-of-decimal-places-in-python", "title": "How to convert float to string with specific number of decimal places in Python polars?", "body": "<p>I have a polars DataFrame with multiple numeric (float dtype) columns. I want to write some of them to a csv with a certain number of decimal places. The number of decimal places I want is column-specific.</p>\n<p><code>polars</code> offers <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.format.html#polars-format\" rel=\"nofollow noreferrer\">format</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\ndf = pl.DataFrame({&quot;a&quot;: [1/3, 1/4, 1/7]})\n\ndf.select(\n    [\n        pl.format(&quot;as string {}&quot;, pl.col(&quot;a&quot;)),\n        ]\n    )\n\nshape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 literal                       \u2502\n\u2502 ---                           \u2502\n\u2502 str                           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 as string 0.3333333333333333  \u2502\n\u2502 as string 0.25                \u2502\n\u2502 as string 0.14285714285714285 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>However, if I try to set a directive to specify number of decimal places, it fails:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.select(\n    [\n        pl.format(&quot;{:.3f}&quot;, pl.col(&quot;a&quot;)),\n        ]\n)\n</code></pre>\n<blockquote>\n<p>ValueError: number of placeholders should equal the number of arguments</p>\n</blockquote>\n<p>Is there an option to have &quot;real&quot; f-string functionality without using an <code>apply</code>?</p>\n<ul>\n<li><code>pl.__version__: '0.16.16'</code></li>\n<li>related: <a href=\"https://stackoverflow.com/q/71790235/10197418\">Polars: switching between dtypes within a DataFrame</a></li>\n<li>to set the decimal places of <em>all</em> output columns, <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.write_csv.html#polars-dataframe-write-csv\" rel=\"nofollow noreferrer\">pl.DataFrame.write_csv</a> offers the <code>float_precision</code> keyword</li>\n</ul>\n"}, {"tags": ["python", "dataframe", "amazon-s3", "python-polars"], "owner": {"account_id": 3012144, "reputation": 2686, "user_id": 5060792, "user_type": "registered", "accept_rate": 53, "profile_image": "https://i.sstatic.net/Fac5h.jpg?s=256", "display_name": "Clay", "link": "https://stackoverflow.com/users/5060792/clay"}, "is_answered": true, "view_count": 1141, "answer_count": 1, "score": 7, "last_activity_date": 1709715655, "creation_date": 1668306144, "last_edit_date": 1668529746, "question_id": 74418033, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74418033/use-polars-to-read-many-small-json-files-from-s3-in-parallel", "title": "Use polars to read many small json files from S3 in parallel", "body": "<p>I have seen the following polars documentation:<br />\n<a href=\"https://pola-rs.github.io/polars-book/user-guide/multiple_files/intro.html#reading-and-processing-in-parallel\" rel=\"noreferrer\">https://pola-rs.github.io/polars-book/user-guide/multiple_files/intro.html#reading-and-processing-in-parallel</a></p>\n<p>Is there a way to create a query plan to read many small json files from an S3 bucket?</p>\n<p>This would be similar to the way Spark can read many small json files or csv files from a single S3 prefix (path) with</p>\n<pre><code>spark.read.format(&quot;json&quot;).load(&quot;s3a://my-bucket/path/to/smallfiles/*.json&quot;)\n</code></pre>\n"}, {"tags": ["python", "out-of-memory", "python-polars"], "owner": {"account_id": 19473966, "reputation": 412, "user_id": 14245686, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiLxJC-AVd1x70cYC8k15CCnN_h07fVL3j2jHIZ=k-s256", "display_name": "stressed", "link": "https://stackoverflow.com/users/14245686/stressed"}, "is_answered": true, "view_count": 1404, "answer_count": 1, "score": 7, "last_activity_date": 1709380838, "creation_date": 1687720821, "last_edit_date": 1687786197, "question_id": 76552069, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76552069/reduce-polars-memory-consumption-in-unique", "title": "Reduce polars memory consumption in unique()", "body": "<p>I have a dataset that fits into RAM, but causes an out of memory error when I run certain methods, such as <code>df.unique()</code>. My laptop has 16GB of RAM. I am running WSL with 14GB of RAM. I am using Polars version 0.18.4. Running <code>df.estimated_size()</code> says that my dataset is around 6GBs when I read it in. The schema of my data is</p>\n<pre><code>index: Int64\nfirst_name: Utf8\nlast_name: Utf8\nrace: Utf8\npct_1: Float64\npct_2: Float64\npct_3: Float64\npct_4: Float64\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>size = pl.read_parquet(&quot;data.parquet&quot;).estimated_size()\ndf = pl.scan_parquet(&quot;data.parquet&quot;) # use LazyFrames\n</code></pre>\n<p>However, I am unable to perform tasks such as <code>.unique()</code>, <code>.drop_nulls()</code>, and so on without getting SIGKILLed. I am using LazyFrames.</p>\n<p>For example,</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.drop_nulls().collect(streaming=True)\n</code></pre>\n<p>results in an out of memory error. I am able to sidestep this by writing a custom function.</p>\n<pre class=\"lang-py prettyprint-override\"><code>def iterative_drop_nulls(expr: pl.Expr, subset: list[str]) -&gt; pl.LazyFrame:\n    for col in subset:\n        expr = expr.filter(~pl.col(col).is_null())\n\n    return expr\n\ndf = df.pipe(iterative_drop_nulls, [&quot;col1&quot;, &quot;col2&quot;]).collect()\n</code></pre>\n<p>I am quite curious why the latter works but not the former, given that the largest version of the dataset (when I read it in initially) fits into RAM.</p>\n<p>Unfortunately, I am unable to think of a similar trick to do the same thing as <code>.unique()</code>. Is there something I can do to make <code>.unique()</code> take less memory? I have tried:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.lazy().unique(cols).collect(streaming=True)\n</code></pre>\n<p>and</p>\n<pre class=\"lang-py prettyprint-override\"><code>def unique(df: pl.DataFrame, subset: list[str], n_rows: int = 100_000) -&gt; pl.DataFrame:\n    parts = []\n    for slice in df.iter_slices(n_rows=n_rows):\n        parts.append(df.unique(slice, subset=subset))\n\n    return pl.concat(parts)\n</code></pre>\n<p>Edit:</p>\n<p>I would love a better answer, but for now I am using</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.from_pandas(\n    df.collect()\n    .to_pandas()\n    .drop_duplicates(subset=[&quot;col1&quot;, &quot;col2&quot;])\n)\n</code></pre>\n<p>In general I have found Polars to be more memory efficient than Pandas, but maybe this is an area Polars could improve? Curiously, if I use</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.from_pandas(\n    df.collect()\n    .to_pandas(use_pyarrow_extension_array=True)\n    .drop_duplicates(subset=[&quot;col1&quot;, &quot;col2&quot;])\n)\n</code></pre>\n<p>I get the same memory error, so maybe this is a Pyarrow thing.</p>\n"}, {"tags": ["python", "numpy", "rust", "pyo3", "python-polars"], "owner": {"account_id": 24376323, "reputation": 311, "user_id": 18308621, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3978fc91d508b070914ca2bfe96059c9?s=256&d=identicon&r=PG", "display_name": "Hakase", "link": "https://stackoverflow.com/users/18308621/hakase"}, "is_answered": true, "view_count": 2091, "answer_count": 1, "score": 7, "last_activity_date": 1714142080, "creation_date": 1647431277, "question_id": 71496561, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71496561/how-to-exchange-polars-dataframe-between-rust-and-python", "title": "How to exchange Polars-DataFrame between Rust and Python", "body": "<p>I want to write a <code>Python extension</code> using <code>Rust</code> with <code>Ctypes</code> or <code>Pyo3</code> to get better performance than native Python. But how to exchange data such as <code>Polars</code> <code>DataFrame</code> or <code>ndarray</code> type between Rust and Python?</p>\n"}, {"tags": ["python", "python-polars", "rust-polars"], "owner": {"account_id": 1582517, "reputation": 1160, "user_id": 1467552, "user_type": "registered", "accept_rate": 75, "profile_image": "https://www.gravatar.com/avatar/1e869ccb9adbad892241ff1956ca02de?s=256&d=identicon&r=PG", "display_name": "barak1412", "link": "https://stackoverflow.com/users/1467552/barak1412"}, "is_answered": false, "view_count": 646, "answer_count": 0, "score": 7, "last_activity_date": 1718790547, "creation_date": 1701153289, "question_id": 77561774, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77561774/how-does-polars-auto-cache-mechanism-work-on-lazyframes", "title": "How does Polars auto-cache mechanism work on LazyFrames?", "body": "<p>As stated <a href=\"https://twitter.com/RitchieVink/status/1570477690297147392/photo/1\" rel=\"noreferrer\">here</a>, Polars introduced an auto-cache mechanism for LazyFrames that occures multiple times in the logical plan, so the user will not have to actively perform the cache.<br />\nHowever, while trying to examine their new mechanism, I encountred scenarios that the auto-cache is not performed optimally:</p>\n<p><strong>Without explicit cache:</strong></p>\n<pre><code>import polars as pl\n\ndf1 = pl.DataFrame({'id': [0,5,6]}).lazy()\ndf2 = pl.DataFrame({'id': [0,8,6]}).lazy()\ndf3 = pl.DataFrame({'id': [7,8,6]}).lazy()\n\ndf4 = df1.join(df2, on='id')\nprint(pl.concat([df4.join(df3, on='id'), df1,\n                 df4]).explain())\n</code></pre>\n<p>We get the logical plan:</p>\n<pre><code>UNION\n  PLAN 0:\n    INNER JOIN:\n    LEFT PLAN ON: [col(&quot;id&quot;)]\n      INNER JOIN:\n      LEFT PLAN ON: [col(&quot;id&quot;)]\n        CACHE[id: a4bcf9591fefc837, count: 3]\n          DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n      RIGHT PLAN ON: [col(&quot;id&quot;)]\n        CACHE[id: 8cee8e3a6f454983, count: 1]\n          DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n      END INNER JOIN\n    RIGHT PLAN ON: [col(&quot;id&quot;)]\n      DF [&quot;id&quot;]; PROJECT */1 COLUMNS; SELECTION: &quot;None&quot;\n    END INNER JOIN\n  PLAN 1:\n    CACHE[id: a4bcf9591fefc837, count: 3]\n      DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n  PLAN 2:\n    INNER JOIN:\n    LEFT PLAN ON: [col(&quot;id&quot;)]\n      CACHE[id: a4bcf9591fefc837, count: 3]\n        DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n    RIGHT PLAN ON: [col(&quot;id&quot;)]\n      CACHE[id: 8cee8e3a6f454983, count: 1]\n        DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n    END INNER JOIN\nEND UNION\n</code></pre>\n<p><strong>With explicit cache:</strong></p>\n<pre><code>import polars as pl\n\ndf1 = pl.DataFrame({'id': [0,5,6]}).lazy()\ndf2 = pl.DataFrame({'id': [0,8,6]}).lazy()\ndf3 = pl.DataFrame({'id': [7,8,6]}).lazy()\n\ndf4 = df1.join(df2, on='id').cache()\nprint(pl.concat([df4.join(df3, on='id'), df1,\n                 df4]).explain())\n</code></pre>\n<p>We get the logical plan:</p>\n<pre><code>UNION\n  PLAN 0:\n    INNER JOIN:\n    LEFT PLAN ON: [col(&quot;id&quot;)]\n      CACHE[id: 290661b0780, count: 18446744073709551615]\n        FAST_PROJECT: [id]\n          INNER JOIN:\n          LEFT PLAN ON: [col(&quot;id&quot;)]\n            DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n          RIGHT PLAN ON: [col(&quot;id&quot;)]\n            DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n          END INNER JOIN\n    RIGHT PLAN ON: [col(&quot;id&quot;)]\n      DF [&quot;id&quot;]; PROJECT */1 COLUMNS; SELECTION: &quot;None&quot;\n    END INNER JOIN\n  PLAN 1:\n    DF [&quot;id&quot;]; PROJECT */1 COLUMNS; SELECTION: &quot;None&quot;\n  PLAN 2:\n    CACHE[id: 290661b0780, count: 18446744073709551615]\n      FAST_PROJECT: [id]\n        INNER JOIN:\n        LEFT PLAN ON: [col(&quot;id&quot;)]\n          DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n        RIGHT PLAN ON: [col(&quot;id&quot;)]\n          DF [&quot;id&quot;]; PROJECT 1/1 COLUMNS; SELECTION: &quot;None&quot;\n        END INNER JOIN\nEND UNION\n</code></pre>\n<p>You can see, that with the explicit cache, we get more optimal plan because the join of <code>df1</code> and <code>df2</code> is performed only once.</p>\n<p>Why doesn't Polars auto-cache mechanism detect the repeated usage of join, and apply the cache by itself? What am I missing?</p>\n<p>Thanks.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 1605734, "reputation": 9214, "user_id": 1485877, "user_type": "registered", "accept_rate": 63, "profile_image": "https://i.sstatic.net/tkdN8.jpg?s=256", "display_name": "drhagen", "link": "https://stackoverflow.com/users/1485877/drhagen"}, "is_answered": true, "view_count": 8452, "answer_count": 4, "score": 6, "last_activity_date": 1706708354, "creation_date": 1648258561, "last_edit_date": 1703837272, "question_id": 71624674, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71624674/make-a-constant-column-in-polars", "title": "Make a constant column in Polars", "body": "<p>In Polars 0.13.14, I could create a <code>DataFrame</code> with an all-constant column like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\n\npl.DataFrame(dict(x=pl.repeat(1, 3)))\n\n# shape: (3, 1)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 x   \u2502\n# \u2502 --- \u2502\n# \u2502 i64 \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 1   \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 1   \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>But in Polars 0.13.15, this is an error</p>\n<pre><code>ValueError: Series constructor not called properly.\n</code></pre>\n<p>How do I fill a column with a value in polars?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 7276158, "reputation": 1024, "user_id": 5547553, "user_type": "registered", "accept_rate": 82, "profile_image": "https://www.gravatar.com/avatar/b717b20c9bba75432c27d8d550f81caf?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "lmocsi", "link": "https://stackoverflow.com/users/5547553/lmocsi"}, "is_answered": true, "view_count": 2983, "accepted_answer_id": 76207392, "answer_count": 2, "score": 6, "last_activity_date": 1724162849, "creation_date": 1683621407, "question_id": 76207341, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76207341/how-to-get-first-n-chars-from-a-str-column-in-python-polars", "title": "How to get first n chars from a str column in python polars?", "body": "<p>What's the alternative of pandas :</p>\n<pre><code>data['ColumnA'].str[:2]\n</code></pre>\n<p>in python polars?</p>\n<pre><code>pl.col('ColumnA').str[:3]\n</code></pre>\n<p>throws <code>TypeError: 'ExprStringNameSpace' object is not subscriptable\n</code>\nerror.</p>\n"}, {"tags": ["python", "python-3.x", "pandas", "python-polars"], "owner": {"account_id": 9216450, "reputation": 63, "user_id": 6847405, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/656a11fc0b8ac00b707743627b89240b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "lauhoman", "link": "https://stackoverflow.com/users/6847405/lauhoman"}, "is_answered": true, "view_count": 3242, "accepted_answer_id": 73043687, "answer_count": 1, "score": 6, "last_activity_date": 1658266300, "creation_date": 1658219169, "question_id": 73033580, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73033580/why-polars-scan-csv-is-even-faster-than-disk-reading-speed", "title": "Why polars.scan_csv is even faster than disk reading speed?", "body": "<p>I am testing <em>polars</em> performance by <code>LazyDataFrame</code> API <code>polars.scan_csv</code> with <code>filter</code>. The performance is much better than I expect. Filtering a CSV file is even faster than the disk speed!  WHY???</p>\n<p>The CSV file is about <code>1.51 GB</code> on my PC HDD.</p>\n<p>testing code:</p>\n<pre><code>import polars as pl\nt0 = time.time()\nlazy_df = pl.scan_csv(&quot;kline.csv&quot;)\ndf = lazy_df.filter(pl.col('ts') == '2015-01-01').collect().to_pandas()\nprint(time.time() - t0)\n\n&gt; Output: 1.8616907596588135\n</code></pre>\n<p>It takes less than <code>2 seconds</code> to scan the whole CSV file, which means that the scan speed is faster than <code>750MB/S</code>. It is much faster than the disk speed, apparently.</p>\n"}, {"tags": ["python", "indexing", "row", "expression", "python-polars"], "owner": {"account_id": 24328613, "reputation": 71, "user_id": 18267812, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyjPANzPsyKoCTVoyQL8ZYYK_VVuJf3j_BgHhg=k-s256", "display_name": "young hwan Song", "link": "https://stackoverflow.com/users/18267812/young-hwan-song"}, "is_answered": true, "view_count": 8750, "answer_count": 2, "score": 6, "last_activity_date": 1723636393, "creation_date": 1680686688, "question_id": 75937764, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75937764/polars-how-to-replace-the-value-of-a-specific-row-and-column-in-df-ex-df3-4", "title": "Polars - How to replace the value of a specific row and column in df (ex. df[3,4]=12) with Polars Expression?", "body": "<p>I know that Polars Expressions are column-wise parallelized for reading and calculating and input/change data in a DataFrame.\nSo, it's very fast...</p>\n<p>However, I cannot find the <strong>Polars Expression</strong> that change a specific value of row and column as <code>df[1,'a']=12</code>\nHow can I do it?</p>\n<p>Below is how I would like to do it.</p>\n<pre><code>import polars as pl\ndf = pl.DataFrame(\n    {\n        &quot;a&quot;: [1,2,3,4,5,6,7,8],\n        &quot;b&quot;: [8,9,0,1,2,3,4,5],\n        &quot;c&quot;: [0,0,0,0,0,0,0,0],\n        &quot;d&quot;: [0,0,0,0,0,0,0,0],\n    }\n)\ndf = df.with_columns(pl.all().cast(pl.Float64))\n\ndf[7,'c']=12   \ndf[7,'d']=15\n# I want this to below way that Polars Expressions\n# =&gt; df.select(?)(\n        pl.col('c').row(7) = 12\n        pl.col('d').row(7) = 15\n     )\n\nprint(df)\n</code></pre>\n<pre><code>shape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c    \u2506 d    \u2502\n\u2502 --- \u2506 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 f64 \u2506 f64 \u2506 f64  \u2506 f64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0 \u2506 8.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 2.0 \u2506 9.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 3.0 \u2506 0.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 4.0 \u2506 1.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 5.0 \u2506 2.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 6.0 \u2506 3.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 7.0 \u2506 4.0 \u2506 0.0  \u2506 0.0  \u2502\n\u2502 8.0 \u2506 5.0 \u2506 12.0 \u2506 15.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n"}, {"tags": ["python-3.x", "dataframe", "python-polars"], "owner": {"account_id": 13732367, "reputation": 421, "user_id": 9909598, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/cc2abd340179a34f05ba9ecc29dce4de?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "DataWiz", "link": "https://stackoverflow.com/users/9909598/datawiz"}, "is_answered": true, "view_count": 10254, "accepted_answer_id": 72118721, "answer_count": 3, "score": 6, "last_activity_date": 1677759683, "creation_date": 1651665202, "question_id": 72112532, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72112532/optimal-way-to-get-all-duplicated-rows-in-a-polars-dataframe", "title": "Optimal way to get all duplicated rows in a polars dataframe", "body": "<p>I want to filter all duplicated rows from a polars dataframe. What I've tried:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame([['1', '1', '1', '1'], ['7', '7', '2', '7'], ['3', '9', '3', '9']])\ndf\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_0 \u2506 column_1 \u2506 column_2 \u2502\n\u2502 ---      \u2506 ---      \u2506 ---      \u2502\n\u2502 str      \u2506 str      \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 7        \u2506 3        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1        \u2506 7        \u2506 9        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1        \u2506 2        \u2506 3        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1        \u2506 7        \u2506 9        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\ndf.filter(pl.all().is_duplicated())\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_0 \u2506 column_1 \u2506 column_2 \u2502\n\u2502 ---      \u2506 ---      \u2506 ---      \u2502\n\u2502 str      \u2506 str      \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 7        \u2506 3        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1        \u2506 7        \u2506 9        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1        \u2506 7        \u2506 9        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>This selects the first row, because it appears to go column-by-column and returns each row where all columns have a corresponding duplicate in the respective column - not the intended outcome.</p>\n<p>Boolean indexing works:</p>\n<pre><code>df[df.is_duplicated(), :]\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_0 \u2506 column_1 \u2506 column_2 \u2502\n\u2502 ---      \u2506 ---      \u2506 ---      \u2502\n\u2502 str      \u2506 str      \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 7        \u2506 9        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1        \u2506 7        \u2506 9        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>But it leaves me wondering</p>\n<ul>\n<li>if this is indeed the only way to do it,</li>\n<li>if there's a way to use <code>.filter()</code> and expressions to achieve the same result</li>\n<li>if this is the most efficient way to achieve the desired result</li>\n</ul>\n"}, {"tags": ["python", "max", "python-polars"], "owner": {"account_id": 452570, "reputation": 8504, "user_id": 849076, "user_type": "registered", "accept_rate": 77, "profile_image": "https://www.gravatar.com/avatar/e0ef1dab4fd1605b186d70cb5f1f06a9?s=256&d=identicon&r=PG", "display_name": "leo", "link": "https://stackoverflow.com/users/849076/leo"}, "is_answered": true, "view_count": 807, "accepted_answer_id": 77967494, "answer_count": 3, "score": 6, "last_activity_date": 1707774494, "creation_date": 1707473352, "last_edit_date": 1707774494, "question_id": 77967334, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77967334/getting-min-max-column-name-in-polars", "title": "Getting min/max column name in Polars", "body": "<p>In polars I can get the horizontal max (maximum value of a set of columns for reach row) like this:</p>\n<pre><code>df = pl.DataFrame(\n    {\n        &quot;a&quot;: [1, 8, 3],\n        &quot;b&quot;: [4, 5, None],\n    }\n)\n\ndf.with_columns(max = pl.max_horizontal(&quot;a&quot;, &quot;b&quot;))\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b    \u2506 max \u2502\n\u2502 --- \u2506 ---  \u2506 --- \u2502\n\u2502 i64 \u2506 i64  \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4    \u2506 4   \u2502\n\u2502 8   \u2506 5    \u2506 8   \u2502\n\u2502 3   \u2506 null \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>This corresponds to Pandas <code>df[[&quot;a&quot;, &quot;b&quot;]].max(axis=1)</code>.</p>\n<p>Now, how do I get the column names instead of the actual max value?\nIn other words, what is the Polars version of Pandas' <code>df[CHANGE_COLS].idxmax(axis=1)</code>?</p>\n<p>The expected output would be:</p>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b    \u2506 max \u2502\n\u2502 --- \u2506 ---  \u2506 --- \u2502\n\u2502 i64 \u2506 i64  \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 4    \u2506 b   \u2502\n\u2502 8   \u2506 5    \u2506 a   \u2502\n\u2502 3   \u2506 null \u2506 a   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 7330809, "reputation": 516, "user_id": 5583772, "user_type": "registered", "accept_rate": 100, "profile_image": "https://www.gravatar.com/avatar/2a3b9faf0f07cb61c92be92ee4f5a096?s=256&d=identicon&r=PG", "display_name": "Paul Fleming", "link": "https://stackoverflow.com/users/5583772/paul-fleming"}, "is_answered": true, "view_count": 4149, "accepted_answer_id": 76078016, "answer_count": 2, "score": 6, "last_activity_date": 1710092312, "creation_date": 1682135715, "last_edit_date": 1709886746, "question_id": 76077950, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76077950/how-do-you-copy-a-dataframe-in-polars", "title": "How do you copy a dataframe in polars?", "body": "<p>In polars, what is the way to make a copy of a dataframe?  In pandas it would be:</p>\n<pre><code>df_copy = df.copy()\n</code></pre>\n<p>But what is the syntax for polars?</p>\n"}, {"tags": ["python-polars"], "owner": {"account_id": 2019981, "reputation": 2274, "user_id": 1806392, "user_type": "registered", "accept_rate": 59, "profile_image": "https://i.sstatic.net/m96tp.jpg?s=256", "display_name": "nik", "link": "https://stackoverflow.com/users/1806392/nik"}, "is_answered": true, "view_count": 3877, "accepted_answer_id": 74994913, "answer_count": 3, "score": 6, "last_activity_date": 1710447872, "creation_date": 1672748950, "question_id": 74993391, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74993391/use-a-list-of-filters-within-polars", "title": "Use a list of filters within polars", "body": "<p>Is there a way to filter a polars DataFrame by multiple conditions?</p>\n<p>This is my use case and how I currently solve it, but I wonder how to solve it, if my list of dates would be longer:</p>\n<pre><code>dates = [&quot;2018-03-25&quot;, &quot;2019-03-31&quot;, &quot;2020-03-29&quot;]\ntimechange_forward = [(datetime.strptime(x+&quot;T02:00&quot;, '%Y-%m-%dT%H:%M'), datetime.strptime(x+&quot;T03:01&quot;, '%Y-%m-%dT%H:%M')) for x in dates]\n\ndf.filter(\n    pl.col(&quot;time&quot;).is_between(*timechange_forward[0]) | \n    pl.col(&quot;time&quot;).is_between(*timechange_forward[1]) | \n    pl.col(&quot;time&quot;).is_between(*timechange_forward[2])\n) \n</code></pre>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 8061961, "reputation": 1003, "user_id": 6077239, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d64f3a25c9fd26ad2f8984239cfa1b97?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "lebesgue", "link": "https://stackoverflow.com/users/6077239/lebesgue"}, "is_answered": true, "view_count": 1098, "accepted_answer_id": 75216101, "answer_count": 1, "score": 6, "last_activity_date": 1680229587, "creation_date": 1674513592, "question_id": 75215780, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75215780/implement-qcut-functionality-using-polars", "title": "Implement qcut functionality using polars", "body": "<p>I have been using polars but it seems like it lacks qcut functionality as pandas do.</p>\n<p>I am not sure about the reason but is it possible to achieve the same effect as pandas qcut using current available polars functionalities?</p>\n<p>The following shows an example about what I can do with pandas qcut.</p>\n<pre><code>import pandas as pd\n\ndata = pd.Series([11, 1, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5])\npd.qcut(data, [0, 0.2, 0.4, 0.6, 0.8, 1], labels=['q1', 'q2', 'q3', 'q4', 'q5'])\n</code></pre>\n<p>The results are as follows:</p>\n<pre><code>0     q5\n1     q1\n2     q1\n3     q1\n4     q3\n5     q4\n6     q5\n7     q1\n8     q1\n9     q3\n10    q4\n11    q5\ndtype: category\n</code></pre>\n<p>So, I am curious how can I get the same result by using polars?</p>\n<p>Thanks for your help.</p>\n"}, {"tags": ["python", "dictionary", "python-polars"], "owner": {"account_id": 29092899, "reputation": 61, "user_id": 22286335, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b51c4f86fc7904a91d92a0ab2e432b86?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user22286335", "link": "https://stackoverflow.com/users/22286335/user22286335"}, "is_answered": true, "view_count": 823, "accepted_answer_id": 77523227, "answer_count": 2, "score": 6, "last_activity_date": 1723657983, "creation_date": 1700569090, "last_edit_date": 1716374087, "question_id": 77522753, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77522753/how-to-use-to-dict-and-orient-records-in-polars-that-is-being-used-in-pandas", "title": "How to use to_dict and orient=&#39;records&#39; in Polars that is being used in pandas?", "body": "<p>Using polars, I am not getting the same output as pandas when calling <code>to_dict</code>.</p>\n<p><strong>Pandas.</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame({\n    'column_1': [1, 2, 1, 4, 5],\n    'column_2': ['Alice', 'Bob', 'Alice', 'Tom', 'Tom'],\n    'column_3': ['Alice1', 'Bob', 'Alice2', 'Tom', 'Tom']\n})\n\ndf.to_dict(orient='records')\n</code></pre>\n<p>produces</p>\n<pre><code>[{'column_1': 1, 'column_2': 'Alice', 'column_3': 'Alice1'}, {'column_1': 2, 'column_2': 'Bob', 'column_3': 'Bob'}, {'column_1': 1, 'column_2': 'Alice', 'column_3': 'Alice2'}, {'column_1': 4, 'column_2': 'Tom', 'column_3': 'Tom'}, {'column_1': 5, 'column_2': 'Tom', 'column_3': 'Tom'}]\n</code></pre>\n<p><strong>Polars.</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame({\n    'column_1': [1, 2, 1, 4, 5],\n    'column_2': ['Alice', 'Bob', 'Alice', 'Tom', 'Tom'],\n    'column_3': ['Alice1', 'Bob', 'Alice2', 'Tom', 'Tom']\n})\n\ndf.to_dict(as_series=False)\n</code></pre>\n<p>produces</p>\n<pre><code>{'column_1': [1, 2, 1, 4, 5], 'column_2': ['Alice', 'Bob', 'Alice', 'Tom', 'Tom'], 'column_3': ['Alice1', 'Bob', 'Alice2', 'Tom', 'Tom']}\n</code></pre>\n<p>Here, the first example is pandas and the output I got when using <code>to_dict</code> with <code>orient='records'</code>. I expected to have the same output in polars.</p>\n<p>How can I replicate pandas' behaviour in polars?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 1970134, "reputation": 577, "user_id": 1769327, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3bfd798965baca97254055e175a537c2?s=256&d=identicon&r=PG", "display_name": "HapiDaze", "link": "https://stackoverflow.com/users/1769327/hapidaze"}, "is_answered": true, "view_count": 1116, "accepted_answer_id": 77160545, "answer_count": 2, "score": 6, "last_activity_date": 1713647800, "creation_date": 1695409016, "last_edit_date": 1696389346, "question_id": 77160103, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77160103/exponential-moving-average-ema-calculations-in-polars-dataframe", "title": "Exponential Moving Average (EMA) calculations in Polars dataframe", "body": "<p>I have the following list of 20 values:</p>\n<pre><code>values = [143.15,143.1,143.06,143.01,143.03,143.09,143.14,143.18,143.2,143.2,143.2,143.31,143.38,143.35,143.34,143.25,143.33,143.3,143.33,143.36]\n</code></pre>\n<p>In order to find the Exponential Moving Average, across a span of 9 values, I can do the following in Python:</p>\n<pre><code>def calculate_ema(values, periods, smoothing=2):\n    ema = [sum(values[:periods]) / periods]\n    \n    for price in values[periods:]:\n        ema.append((price * (smoothing / (1 + periods))) + ema[-1] * (1 - (smoothing / (1 + periods))))\n    return ema\n\nema_9 = calculate_ema(values, periods=9)\n</code></pre>\n<pre><code>[143.10666666666668,\n 143.12533333333334,\n 143.14026666666666,\n 143.17421333333334,\n 143.21537066666667,\n 143.24229653333333,\n 143.26183722666667,\n 143.25946978133334,\n 143.27357582506667,\n 143.27886066005334,\n 143.28908852804267,\n 143.30327082243414]\n</code></pre>\n<p>The resulting list of EMA values is 12 items long, the first value [0] corresponding to the 9th [8] value from <em>values</em>.</p>\n<p>Using Pandas and TA-Lib, I can perform the following:</p>\n<pre><code>import pandas as pd\nimport talib as ta\n\ndf_pan = pd.DataFrame(\n    {\n        'value': values\n    }\n)\n\ndf_pan['ema_9'] = ta.EMA(df_pan['value'], timeperiod=9)\n\ndf_pan\n</code></pre>\n<pre><code>    value   ema_9\n0   143.15  NaN\n1   143.10  NaN\n2   143.06  NaN\n3   143.01  NaN\n4   143.03  NaN\n5   143.09  NaN\n6   143.14  NaN\n7   143.18  NaN\n8   143.20  143.106667\n9   143.20  143.125333\n10  143.20  143.140267\n11  143.31  143.174213\n12  143.38  143.215371\n13  143.35  143.242297\n14  143.34  143.261837\n15  143.25  143.259470\n16  143.33  143.273576\n17  143.30  143.278861\n18  143.33  143.289089\n19  143.36  143.303271\n</code></pre>\n<p>The Pandas / TA-Lib output corresponds with that of my Python function.</p>\n<p>However, when I try to replicate this using funtionality purely in Polars:</p>\n<pre><code>import polars as pl\n\ndf = (\n    pl.DataFrame(\n        {\n            'value': values\n        }\n    )\n    .with_columns(\n        pl.col('value').ewm_mean(span=9, min_periods=9,).alias('ema_9')\n    )\n)\n\ndf\n</code></pre>\n<p>I get different values:</p>\n<pre><code>value   ema_9\nf64 f64\n143.15  null\n143.1   null\n143.06  null\n143.01  null\n143.03  null\n143.09  null\n143.14  null\n143.18  null\n143.2   143.128695\n143.2   143.144672\n143.2   143.156777\n143.31  143.189683\n143.38  143.229961\n143.35  143.255073\n143.34  143.272678\n143.25  143.268011\n143.33  143.280694\n143.3   143.284626\n143.33  143.293834\n143.36  143.307221\n</code></pre>\n<p>Can anyone please explain what adjustments I need to make to my Polars code in order get the expected results?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 28932683, "reputation": 83, "user_id": 22160957, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3de60dcaab0878a518def561f9e0daf9?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Escobar West", "link": "https://stackoverflow.com/users/22160957/escobar-west"}, "is_answered": true, "view_count": 3423, "accepted_answer_id": 78441223, "answer_count": 3, "score": 6, "last_activity_date": 1715071714, "creation_date": 1688255812, "question_id": 76596952, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76596952/how-do-i-select-the-top-k-rows-of-a-python-polars-dataframe-for-each-group", "title": "How do I select the top k rows of a python polars dataframe for each group?", "body": "<p>The polars dataframe has a <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.top_k.html\" rel=\"noreferrer\">top_k</a> method that can be used to select rows which contain the k largest values when sorting on a column. For example, the following code selects the two rows with the largest and second largest entry in the <code>val</code> column:</p>\n<pre><code>df = pl.DataFrame({'grp':['a','a','a','b','b','b'], 'val':[1,2,3,10,20,30], 'etc':[0,1,2,3,4,5]})\n\ngrp val etc\nstr i64 i64\n&quot;a&quot; 1   0\n&quot;a&quot; 2   1\n&quot;a&quot; 3   2\n&quot;b&quot; 10  3\n&quot;b&quot; 20  4\n&quot;b&quot; 30  5\n\ndf.top_k(2, by='val')\n\ngrp val etc\nstr i64 i64\n&quot;b&quot; 30  5\n&quot;b&quot; 20  4\n</code></pre>\n<p>My question is: how do I get the rows with top k values for each group? Specifically, I want the entire row and not just the value in the <code>val</code> column. I want to do something like this, but this doesn't work in polars because polars <code>GroupBy</code> doesn't have a <code>top_k</code> method:</p>\n<pre><code>df.groupby('grp').top_k(2, by='val') # doesnt work in polars\n\ngrp val etc\nstr i64 i64\n&quot;b&quot; 30  5\n&quot;b&quot; 20  4\n&quot;a&quot; 3   2\n&quot;a&quot; 2   1\n</code></pre>\n<p>I was able to come up with two ways: one using <code>apply</code> and another using sorting. Both of these are not desirable for performance reasons. <code>apply</code> is generally not recommended because <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.dataframe.groupby.GroupBy.apply.html#polars.dataframe.groupby.GroupBy.apply\" rel=\"noreferrer\">it's almost always significantly slower</a>. The sorting option is also not desirable as getting the top k elements uses a faster algorithm than sorting (for small k and large n, it's basically O(n) vs O(n log n)). So even though the following below work, I'm looking for other approaches. Is there any way to directly use a <code>top_k</code> method with polars groupby? That would be my ideal solution.</p>\n<pre><code># works, but at expense of using apply method\ndf.groupby('grp').apply(lambda df: df.top_k(2, by='val'))\n\ngrp val etc\nstr i64 i64\n&quot;b&quot; 30  5\n&quot;b&quot; 20  4\n&quot;a&quot; 3   2\n&quot;a&quot; 2   1\n</code></pre>\n<pre><code># works, but at expense of sorting entire groups\ndf.groupby('grp').agg(pl.all().sort_by('val', descending=True).head(2)).explode('val','etc')\n\ngrp val etc\nstr i64 i64\n&quot;a&quot; 3   2\n&quot;a&quot; 2   1\n&quot;b&quot; 30  5\n&quot;b&quot; 20  4\n</code></pre>\n<ul>\n<li><code>df.groupby('grp').top_k(2, by='val')</code>, which doesn't work in polars</li>\n<li><code>df.groupby('grp').apply(lambda df: df.top_k(2, by='val'))</code>, which works at the cost of using <code>apply</code></li>\n<li><code>df.groupby('grp').agg(pl.all().sort_by('val', descending=True).head(2)).explode('val','etc')</code>, which works at the cost of sorting</li>\n</ul>\n"}, {"tags": ["python", "dataframe", "python-polars"], "owner": {"account_id": 6745226, "reputation": 2423, "user_id": 5197034, "user_type": "registered", "accept_rate": 78, "profile_image": "https://www.gravatar.com/avatar/d00fb325a5fdd7ae0b615cd23d20f780?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "pietz", "link": "https://stackoverflow.com/users/5197034/pietz"}, "is_answered": true, "view_count": 7813, "answer_count": 2, "score": 6, "last_activity_date": 1720790383, "creation_date": 1676372510, "question_id": 75446886, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75446886/selecting-columns-based-on-a-condition-in-polars", "title": "Selecting columns based on a condition in Polars", "body": "<p>I want to select columns in a Polars DataFrame based on a condition. In my case, I want to select all string columns that have less than 100 unique values. Naively I tried:</p>\n<pre><code>df.select((pl.col(pl.Utf8)) &amp; (pl.all().n_unique() &lt; 100))\n</code></pre>\n<p>which gave me an error, which is probably due to the second part of the expression.</p>\n<pre><code>df.select(pl.all().n_unique() &lt; 100)\n</code></pre>\n<p>This doesn't select columns but instead returns a single row DataFrame of bool values. I'm new to polars and still can't quite wrap my head around the expression API, I guess. What am I doing wrong?</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 16165356, "reputation": 359, "user_id": 11670455, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-uU-3uc08kgY/AAAAAAAAAAI/AAAAAAAAACk/lCqp3Jr-xiw/photo.jpg?sz=256", "display_name": "Frank Jimenez", "link": "https://stackoverflow.com/users/11670455/frank-jimenez"}, "is_answered": true, "view_count": 5573, "accepted_answer_id": 75164386, "answer_count": 2, "score": 6, "last_activity_date": 1699914835, "creation_date": 1674070123, "question_id": 75164370, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75164370/python-polars-how-to-convert-a-list-of-dictionaries-to-polars-dataframe-without", "title": "Python Polars: how to convert a list of dictionaries to polars dataframe without using pandas", "body": "<p>I have a list of dictionaries like this:</p>\n<pre><code>[{&quot;id&quot;: 1, &quot;name&quot;: &quot;Joe&quot;, &quot;lastname&quot;: &quot;Bloggs&quot;}, {&quot;id&quot;: 2, &quot;name&quot;: &quot;Bob&quot;, &quot;lastname&quot;: &quot;Wilson&quot;}]\n</code></pre>\n<p>And I would like to transform it to a polars dataframe. I've tried going via pandas but if possible, I'd like to avoid using pandas.</p>\n<p>Any thoughts?</p>\n"}, {"tags": ["python-polars", "rust-polars"], "owner": {"account_id": 4288150, "reputation": 438, "user_id": 3505206, "user_type": "registered", "profile_image": "https://i.sstatic.net/OhSzz.png?s=256", "display_name": "Jenobi", "link": "https://stackoverflow.com/users/3505206/jenobi"}, "is_answered": true, "view_count": 10551, "accepted_answer_id": 72564488, "answer_count": 2, "score": 6, "last_activity_date": 1725273547, "creation_date": 1654791473, "question_id": 72563617, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72563617/fast-apply-of-a-function-to-polars-dataframe", "title": "Fast apply of a function to Polars Dataframe", "body": "<p>What are the fastest ways to apply functions to polars DataFrames - <code>pl.DataFrame</code> or <code>pl.internals.lazy_frame.LazyFrame</code>? This question is piggy-backing off <a href=\"https://stackoverflow.com/questions/67834912/apply-function-to-all-columns-of-a-polars-dataframe\">Apply Function to all columns of a Polars-DataFrame</a></p>\n<p>I am trying to concat all columns and hash the value using hashlib in python standard library. The function I am using is below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import hashlib\n\ndef hash_row(row):\n    os.environ['PYTHONHASHSEED'] = &quot;0&quot;\n    row = str(row).encode('utf-8')\n    return hashlib.sha256(row).hexdigest()\n\n</code></pre>\n<p>However given that this function requires a string as input, means this function needs to be applied to every cell within a pl.Series. Working with a small amount of data, should be okay, but when we have closer to 100m rows this becomes very problematic. The question for this thread is how can we apply such a function in the most-performant way across an entire Polars Series?</p>\n<h4>Pandas</h4>\n<p>Offers a few options to create new columns, and some are more performant than others.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['new_col'] = df['some_col'] * 100 # vectorized calls\n\n</code></pre>\n<p>Another option is to create custom functions for row-wise operations.</p>\n<pre class=\"lang-py prettyprint-override\"><code>def apply_func(row):\n   return row['some_col'] + row['another_col']\n\ndf['new_col'] = df.apply(lambda row: apply_func(row), axis=1) # using apply operations\n</code></pre>\n<p>From my experience, the fastest way is to create numpy vectorized solutions.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\n\ndef np_func(some_col, another_col):\n   return some_col + another_col\n\nvec_func = np.vectorize(np_func)\n\ndf['new_col'] = vec_func(df['some_col'].values, df['another_col'].values)\n\n</code></pre>\n<h4>Polars</h4>\n<p>What is the best solution for Polars?</p>\n"}, {"tags": ["pandas", "python-polars"], "owner": {"account_id": 8042203, "reputation": 531, "user_id": 9274940, "user_type": "registered", "profile_image": "https://i.sstatic.net/f68HM.png?s=256", "display_name": "Tonino Fernandez", "link": "https://stackoverflow.com/users/9274940/tonino-fernandez"}, "is_answered": true, "view_count": 3261, "accepted_answer_id": 76828004, "answer_count": 1, "score": 6, "last_activity_date": 1706246812, "creation_date": 1690992301, "question_id": 76821687, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76821687/polars-equivalent-of-values-in-pandas", "title": "Polars equivalent of .values in Pandas", "body": "<p>I want to convert the values of my dataframe into an array using Polars.</p>\n<p>With Pandas I would do this:</p>\n<pre><code>import pandas as pd\n\ndf_tests = pd.DataFrame({'col_a':[1,2,3], 'col_b':[4,5,6]}, 'col_c':[7,8,9]})\nprint(df_tests.values)\n</code></pre>\n<p>What's the equivalent in Polars?</p>\n<p>I've tried the to_list() method, but only works for series. The closest I could get is this, which returns a list of tuples:</p>\n<pre><code>import polars as pl\n\n# Convert Pandas DataFrame to Polars DataFrame\ndf_tests = pd.DataFrame({'col_a':[1,2,3], 'col_b':[4,5,6]}, 'col_c':[7,8,9]})\ndf_tests_pl = pl.from_pandas(df_tests)\n\nprint(df_tests_pl.select(pl.col(['col_a', 'col_b'])).rows())\n</code></pre>\n"}, {"tags": ["python", "pandas", "dataframe", "null", "python-polars"], "owner": {"account_id": 302279, "reputation": 121025, "user_id": 610569, "user_type": "registered", "accept_rate": 90, "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG", "display_name": "alvas", "link": "https://stackoverflow.com/users/610569/alvas"}, "is_answered": true, "view_count": 5462, "accepted_answer_id": 76219963, "answer_count": 2, "score": 6, "last_activity_date": 1694343023, "creation_date": 1683728795, "last_edit_date": 1683729983, "question_id": 76219628, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76219628/how-to-find-the-no-of-nulls-in-every-column-in-a-polars-dataframe", "title": "How to find the no. of nulls in every column in a polars dataframe?", "body": "<p>In pandas, one can do:</p>\n<pre><code>import pandas as pd\n\nd = {&quot;foo&quot;:[1,2,3, None], &quot;bar&quot;:[4,None, None, 6]}\ndf_pandas = pd.DataFrame.from_dict(d)\ndict(df_pandas.isnull().sum())\n</code></pre>\n<p>[out]:</p>\n<pre><code>{'foo': 1, 'bar': 2}\n</code></pre>\n<p>In polars it's possible to do the same by looping through the columns:</p>\n<pre><code>import polars as pl\n\nd = {&quot;foo&quot;:[1,2,3, None], &quot;bar&quot;:[4,None, None, 6]}\ndf_polars = pl.from_dict(d)\n\n{col:df_polars[col].is_null().sum() for col in df_polars.columns}\n</code></pre>\n<p>Looping through the columns in polars is particularly painful when using <code>LazyFrame</code>, then the <code>.collect()</code> has to be done in chunks to do the aggregation.</p>\n<p><strong>Is there a way to find no. of nulls in every column in a polars dataframe without looping through each columns?</strong></p>\n"}, {"tags": ["python-polars", "rust-polars"], "owner": {"account_id": 27600187, "reputation": 63, "user_id": 21065580, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp7lF8U3nmuuJm__cBoo8eSb6I4z7gbkKRfaJNHa=k-s256", "display_name": "Dennis L", "link": "https://stackoverflow.com/users/21065580/dennis-l"}, "is_answered": true, "view_count": 4729, "accepted_answer_id": 75396733, "answer_count": 3, "score": 6, "last_activity_date": 1706158885, "creation_date": 1675325434, "question_id": 75320233, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75320233/polars-dataframe-save-to-sql", "title": "Polars DataFrame save to sql", "body": "<p>Is there a way to save Polars DataFrame into a database, MS SQL for example?</p>\n<p>ConnectorX library doesn\u2019t seem to have that option.</p>\n"}, {"tags": ["python", "python-polars"], "owner": {"account_id": 2142798, "reputation": 2826, "user_id": 1901071, "user_type": "registered", "accept_rate": 90, "profile_image": "https://www.gravatar.com/avatar/fc4b1bc6808a1543de0a6259a67d8f2d?s=256&d=identicon&r=PG", "display_name": "John Smith", "link": "https://stackoverflow.com/users/1901071/john-smith"}, "is_answered": true, "view_count": 3140, "accepted_answer_id": 75309056, "answer_count": 1, "score": 6, "last_activity_date": 1675248479, "creation_date": 1675247917, "question_id": 75308944, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75308944/polars-case-statement", "title": "Polars Case Statement", "body": "<p>I am trying to pick up the package <a href=\"https://www.pola.rs/\" rel=\"noreferrer\">polars</a> from Python.\nI come from an R background so appreciate this might be an incredibly easy question.</p>\n<p>I want to implement a case statement where if any of the conditions below are true, it will flag it to 1 otherwise it will be 0. My new column will be called 'my_new_column_flag'</p>\n<p>I am however getting the error message</p>\n<blockquote>\n<p>Traceback (most recent call last):\nFile &quot;&quot;, line 2, in \nFile &quot;C:\\Users\\foo\\Miniconda3\\envs\\env\\lib\\site-packages\\polars\\internals\\lazy_functions.py&quot;, line 204, in col\nreturn pli.wrap_expr(pycol(name))\nTypeError: argument 'name': 'int' object cannot be converted to 'PyString'</p>\n</blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>import polars as pl\nimport numpy as np\n\nnp.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        &quot;nrs&quot;: [1, 2, 3, None, 5],\n        &quot;names&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;, None],\n        &quot;random&quot;: np.random.rand(5),\n        &quot;groups&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],\n    }\n)\nprint(df)\n\ndf.with_column(\n    pl.when(pl.col('nrs') == 1).then(pl.col(1))\n    .when(pl.col('names') == 'ham').then(pl.col(1))\n    .when(pl.col('random') == 0.014575).then(pl.col(1))\n    .otherwise(pl.col(0))\n    .alias('my_new_column_flag')\n)\n\n</code></pre>\n<p>Can anyone help?</p>\n"}, {"tags": ["python", "append", "python-polars", "writefile"], "owner": {"account_id": 14590538, "reputation": 114, "user_id": 10537826, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b7923ef9aeace8b6215de1025a4048e8?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Mr. Caribbean", "link": "https://stackoverflow.com/users/10537826/mr-caribbean"}, "is_answered": true, "view_count": 4229, "accepted_answer_id": 75273080, "answer_count": 1, "score": 6, "last_activity_date": 1723460018, "creation_date": 1674972248, "last_edit_date": 1685716088, "question_id": 75272909, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75272909/does-polars-module-not-have-a-method-for-appending-dataframes-to-output-files", "title": "Does Polars module not have a method for appending DataFrames to output files?", "body": "<p>When writing a DataFrame to a csv file, I would like to append to the file, instead of overwriting it.</p>\n<p>While pandas DataFrame has the <code>.to_csv()</code> method with the <em>mode</em> parameter available, thus allowing to append the DataFrame to a file,\nNone of the Polars DataFrame write methods seem to have that parameter.</p>\n"}], "has_more": true, "quota_max": 300, "quota_remaining": 299}